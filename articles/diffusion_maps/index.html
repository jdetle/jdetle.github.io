<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <meta name="viewport" content="width=device-width">
    <title>Diffusion Maps in Molecular Dynamics Analysis - John Detlefs' Web Log
    </title>
    <link rel="alternate" href="/feed.xml" type="application/rss+xml" title="a blog for my coding adventures">
    <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic|Anonymous+Pro:400,700,400italic,700italic|Merriweather:400,700,300">
    <link rel="stylesheet" href="/css/main.css">
  </head>
  <body class="article-detail">
    <header class="header">
      <div class="content-wrap">
        <h1>Diffusion Maps in Molecular Dynamics Analysis</h1>
        <p class="author">Written by <span class="author">John Detlefs</span>
        </p>
      </div>
    </header>
    <div id="content">
      <div class="content-wrap">
        <article class="article">
          <section class="content"><script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script>
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<p>It occurs to me in my previous post I didn’t thoroughly explain the motivation
for dimension reduction in general. When we have this data matrix $X$ with $n$
samples and each sample having $m$ features, this number m can be very large.
This data contains information that we want to extract, in the case of molecular
dynamics simulations these are parameters describing how the dynamics are occurring.
But this can be be features that distinguish faces from others in the dataset,
handwritten letters and numbers from other numbers, etc. As it is so eloquently put
by <a href="http://dip.sun.ac.za/~herbst/research/publications/diff_maps_prasa2008.pdf">Porte and Herbst at&nbsp;Arizona</a></p>
<blockquote>
<p>The breakdown of common similarity measures hampers
the efficient organisation of data, which, in turn, has
serious implications in the field of pattern recognition.
For example, consider a collection of n × m images,
each encoding a digit between 0 and 9. Furthermore, the
images differ in their orientation, as shown in Fig.1. A
human, faced with the task of organising such images,
would likely first notice the different digits, and thereafter
that they are oriented. The observer intuitively attaches
greater value to parameters that encode larger variances in the observations,
and therefore clusters the data in 10 groups, one for each&nbsp;digit</p>
</blockquote>
<p>Here we’ve been introduced to the idea of pattern recognition and ‘clustering’,
the latter will be discussed in some detail later. Continuing&nbsp;on…</p>
<blockquote>
<p>On the other hand, a computer sees each image as a
data point in $R^{nm}$, an nm-dimensional coordinate space.
The data points are, by nature, organised according to
their position in the coordinate space, where the most
common similarity measure is the Euclidean&nbsp;distance.</p>
</blockquote>
<p>The idea of the data being in a $nm$ dimensional space is introduced by the
authors. The important part is that a computer has no knowledge of the the patterns
inside this data. The human brain is excellent at plenty of algorithms, but dimension
reduction is one it is especially good&nbsp;at.</p>
<h2 id="start-talking-about-some-chemistry-john-">Start talking about some chemistry&nbsp;John!</h2>
<hr>
<p>Fine! Back to the matter at hand, dimension reduction is an invaluable tool in
modern computational chemistry because of the massive dimensionality of molecular
dynamics simulations. To my knowledge, the biggest things being studied by <span class="caps">MD</span>
currently are on the scale of the <a href="http://www.bio-itworld.com/2013/5/29/researchers-characterize-chemical-structure-hiv-capsid.html"><span class="caps">HIV</span>-1 Capsid</a> at 64 million atoms!
Of course, these studies are being done on supercomputers, and for the most part
studies are running on a much smaller number of atoms. For a thorough explanation
of how <span class="caps">MD</span> simulations work, my Summer of Code colleague <a href="http://fiona-naughton.github.io/blog/2016/05/25/What-is-this-MD-thing-anyway">Fiona Naughton</a>
has an excellent and cat-filled post explaining <span class="caps">MD</span> and Umbrella Sampling. Why do we
care about dynamics? As <a href="https://chemistry.rice.edu/FacultyDetail.aspx?RiceID=238">Dr. Cecilia Clementi</a> mentions in her <a href="http://cgl.uni-jena.de/pub/Workshops/WebHome/CGL_workshop1.pdf">slides</a>,
‘Crystallography gives structures’, but function requires&nbsp;dynamics!’</p>
<p>A molecular dynamics simulation can be thought of as a diffusion process subject
to drag (from the interactions of molecules) and random forces, (brownian motion).
This means that the time evolution of the probability density of a molecule occupying
a point in the configuration space $P(x,t)$ satisfies the <a href="https://en.wikipedia.org/wiki/Fokker%E2%80%93Planck_equation">Fokker-Plank Equation</a>
(This is some complex math from statistical mechanics). The important thing to note
is that the Fokker-Plank equation has a discrete eigenspectrum, and that there
usually exists a spectral gap reflecting the ‘intrinsic dimensionality’ of the
system it is modeling. A diffusion process is by definition markovian, in this case a continuous
markov process, which means the state at time t is solely dependent on the instantaneous
step before it. This is easier when transferred over to the actual discrete problems
in <span class="caps">MD</span> simulation, the state at time $t$ is only determined by the state at time&nbsp;$t-1$.</p>
<p>Diffusion maps in <span class="caps">MD</span> try to find a discrete approximation of the
eigenspectrum of the Fokker-Plank equation by taking the following steps. First, we can think of
changes in configuration as random walks on an infinite graph defined by the
configuration space. <img src="/articles/diffusion_maps/WalksOnGraph.png" alt="graph"> From <a href="http://dip.sun.ac.za/~herbst/research/publications/diff_maps_prasa2008.pdf">Porte</a>&nbsp;again:</p>
<blockquote>
<p>The connectivity between two data points, x and y, is
defined as the probability of jumping from x to y in one
step of the random walk, and&nbsp;is</p>
</blockquote>
<p>$$ connectivity(x,y) = p(x,y)&nbsp;$$</p>
<blockquote>
<p> It is useful to express this connectivity in terms of a non-normalised
likelihood function, k, known as the diffusion&nbsp;kernel:</p>
</blockquote>
<p>$$ connectivity  \propto  k(x,y)&nbsp;$$</p>
<blockquote>
<p>The kernel defines a local measure of similarity within
a certain neighbourhood. Outside the neighbourhood, the
function quickly goes to zero. For example, consider the
popular Gaussian&nbsp;kernel:</p>
</blockquote>
<p>$$ k(x,y) = \exp(-\frac{|x-y|^{2}}{\epsilon})&nbsp;$$</p>
<p><a href="http://cis-linux1.temple.edu/~latecki/Courses/RobotFall08/Papers/DiffusionMaps06.pdf">Coifman and Lafon</a> provide a dense but extremely thorough explanation of diffusion maps in their seminal paper. This quote screams molecular&nbsp;dynamics:</p>
<blockquote>
<p>Now, since the sampling of the data is generally not related to the geometry of the manifold, one would like to recover the manifold structure regardless of the distribution of the data points. In the case when the data points are sampled from the equilibrium distribution of a stochastic dynamical system, the situation is quite different as the density of the points is a quantity of interest, and therefore, cannot be gotten rid of. Indeed, for some dynamical physical systems, regions of high density correspond to minima of the free energy of the system. Consequently, the long-time behavior of the dynamics of this system results in a subtle interaction between the statistics (density) and the geometry of the data&nbsp;set.</p>
</blockquote>
<p>In this paper, the authors acknowledge that oftentimes an isotropic kernel is not sufficient to understand the relationships in the data. He poses the&nbsp;question:</p>
<blockquote>
<p>In particular, what is the influence of the density of the points and of the geometry of the possible underlying data set over the eigenfunctions and spectrum of the diffusion?
 To address this type of question, we now introduce a family of anisotropic diffusion processes that are all obtained as small-scale limits of a graph Laplacian jump process. This family is parameterized by a number $\alpha$ which can be tuned up to specify the amount of influence of the density in the infinitesimal transitions of the diffusion. The crucial point is that the graph Laplacian normalization is not applied on a &gt;graph with isotropic weights, but rather on a renormalized&nbsp;graph.</p>
</blockquote>
<p>The derivation from here requires a few more&nbsp;steps:</p>
<ul>
<li>Form a new kernel from anisotropic diffusion term: Let $$ q_{\epsilon}(x) = \int<em>X k</em>{\epsilon}(x,y)q(y) \,dy$$
$$ k<em>{\epsilon}^{(\alpha)} =  \frac{k</em>{\epsilon}(x,y)}{q<em>{\epsilon}(x) q</em>{\epsilon}(y)&nbsp;}$$</li>
<li>Apply weighted graph Laplacian normalization:
$$ d_{\epsilon}^{(\alpha)}(x) = \int<em>X k</em>{\epsiilon}^{(\alpha)}(x,y)q(y) \,dy&nbsp;$$</li>
<li>Define anisotropic transition kernel from this term
$$ p<em>{\epsilon,\alpha}(x, y) =   \frac{k</em>{\epsiilon}^{(\alpha)}(x,y)}{d_{\epsilon}^{(\alpha)}(x)}$$</li>
</ul>
<p>This was all kinds of painful, but what this means for diffusion maps in <span class="caps">MD</span> is
that a meaningful diffusion map will have an anisotropic, (and therefore unsymmetric kernel).
<a href="http://cis-linux1.temple.edu/~latecki/Courses/RobotFall08/Papers/DiffusionMaps06.pdf">Coifman and Lafon</a> go on to prove that for $\alpha$ equal to $\frac{1}{2}$
this anisotropic kernel is an effective approximation for the Fokker-Plank equation!
This is a really cool result that is in no way&nbsp;obvious.</p>
<p>Originally, when I studied diffusion maps while applying for the Summer of Code
I was completely unaware of Fokker-Plank and the anisotropic kernel. Of course,
learning these topics takes time, but I was under the impression that diffusion
kernels were symmetric across the board, which is just dead wrong. This of course
changes how eigenvalue decomposition can be performed on a matrix and requires a
routine like Singular Value Decomposition instead of Symmetric Eigenvalue Decomposition.
If I had spent more time researching literature on my own I think I could have figured this out
With that being said, there are 100+ dense pages given in the citations&nbsp;below.</p>
<p>So where are we at? Quick recap about diffusion&nbsp;maps:</p>
<ul>
<li>Start taking random walks on a&nbsp;graph</li>
<li>There are different costs for different walks based on likelihood of walk&nbsp;happening</li>
<li>We established a kernel based on all these different&nbsp;walks</li>
<li>For <span class="caps">MD</span> we manipulate this kernel so it is&nbsp;anisotropic!</li>
</ul>
<p>Okay, so what do we have left to talk&nbsp;about…</p>
<ul>
<li>How is epsilon&nbsp;determined?</li>
<li>What if we want to take a random walk of more than one&nbsp;jump?</li>
<li>Hey John, we’re not actually taking random&nbsp;walks!</li>
<li>What do we do once we get an&nbsp;eigenspectrum?</li>
<li>What do we use this&nbsp;for?</li>
</ul>
<h2 id="epsilon-determination">Epsilon&nbsp;Determination</h2>
<p>Epsilon determination is kind of funky. First off, <a href="http://ferguson.matse.illinois.edu/page1/">Dr. Andrew L. Ferguson</a> notes
that division by epsilon retrains ‘only short pairwise distances on the order of $\sqrt{2\epsilon}$’.
In addition, <a href="https://chemistry.rice.edu/FacultyDetail.aspx?RiceID=238">Dr. Clementi</a> in her <a href="http://cgl.uni-jena.de/pub/Workshops/WebHome/CGL_workshop1.pdf">slides</a> on diffusion maps notes that the
neighborhood determined by epsilon should be locally flat. For a free-energy surface, this means that it
is potentially advantageous to define a unique epsilon for every single element of a kernel based on the
nearest neighbors to that point in terms of value. This can get painful.
Most researchers seem to use constant epsilon determined from some sort of guess and check method based on&nbsp;clustering.</p>
<p>For my GSoC pull request that is up right now, the plan is to have an <span class="caps">API</span>
for an <code>Epsilon</code> class that must return a matrix whose $ij th$ coordinate is $\frac{dist(i,j)}{\epsilon_ij }$.
From here, given weights for the anisotropy of the kernel, we can form the anisotropic kernel
to be eigenvalue-decomposed. Any researcher who cares to do some complex choice
of epsilon based on nearest-neighbors is probably a good enough hacker to handle implementation of this <span class="caps">API</span> in a quick&nbsp;script.</p>
<h2 id="length-t-walks">Length $t$&nbsp;Walks</h2>
<p>Nowhere in the construction of our diffusion kernel are we actually taking random walks.
What we are doing is taking all possible walks, where two vertices on the graph are close if $d(x,y)$ is small
and far apart if $d(x,y)$ is large. This accounts for all possible one-step walks across our data.
In order to get a good idea of transitions that occur over larger timesteps, we take multiple steps.
To solve this equation we must multiply our distance matrix $P$ by itself t-times,
where t is the number of steps in the walk across the graph.
From Porte again (stealing is the best form of flattery,&nbsp;no?):</p>
<blockquote>
<p>With increased values of t (i.e. as the diffusion process
“runs forward”), the probability of following a path
along the underlying geometric structure of the data set
increases. This happens because, along the geometric
structure, points are dense and therefore highly connected
(the connectivity is a function of the Euclidean distance
between two points, as discussed in Section 2). Pathways
form along short, high probability jumps. On the other
hand, paths that do not follow this structure include one
or more long, low probability jumps, which lowers the
path’s overall&nbsp;probability.</p>
</blockquote>
<p>I said something blatantly wrong in my last post. I’m a fool, but still, things do
get a little complicated when analyzing time series data with diffusion maps.
We want to both investigate different <em>timescale</em> walks from the diffusion maps,
but also to be able to project our snapshot from a trajectory at a <em>timestep</em>
to the corresponding set of eigenvectors describing the lower dimensional&nbsp;order-parameters.</p>
<p>From <a href="http://ferguson.matse.illinois.edu/page1/">Ferguson</a>:</p>
<blockquote>
<p>The <strong>diffusion map embedding</strong> is defined as the mapping of the
ith snapshot into the ith components of each of the top k non-trivial
eigenvectors of the $M$&nbsp;matrix.</p>
</blockquote>
<p>Here the $M$ matrix is our anisotropic kernel. So from a spectral decomposition
of our kernel (remember that it is generated by a particular <em>timescale</em> walk), we get a
set of eigenvectors that we project our snapshot (what we have been calling a sample)
that exists as a particular <em>timestep</em> in our <span class="caps">MD</span> trajectory. This can create some
overly similar notation, so I’m just going to avoid it and hope that it makes more
sense without&nbsp;notation.</p>
<h2 id="using-diffusion-maps-in-mdanalysis">Using Diffusion Maps in&nbsp;MDAnalysis</h2>
<p>Alright, this has been a lot to digest, but hopefully you are still with me.
Why are we doing this? There are plenty of reasons, and I am going to list a&nbsp;few:</p>
<ul>
<li>Dr. Ferguson used diffusion maps to investigate the assembly of polymer subunits in
<a href="http://ferguson.matse.illinois.edu/resources/10.pdf">this&nbsp;paper</a></li>
<li>Also for the order parameters in <a href="http://ferguson.matse.illinois.edu/resources/2.pdf">alkane chain&nbsp;dynamics</a></li>
<li>Also for <a href="http://ferguson.matse.illinois.edu/resources/4.pdf">umbrella&nbsp;sampling</a></li>
<li>Dr. Clementi used this for protein folding order parameters <a href="https://www.cs.rice.edu/CS/Robotics/papers/das2006low-dim-land-folding-dim-reduction.pdf">here</a></li>
<li>Also, Dr. Clementi used this for polymerization reactions <a href="http://scitation.aip.org/content/aip/journal/jcp/134/14/10.1063/1.3575245">here</a></li>
<li>Dr. Clementi also created a variant that treats epsilon determination very
carefully with <a href="http://www.ncbi.nlm.nih.gov/pubmed/21456654"><span class="caps">LSD</span></a></li>
<li>There are more listed in my works&nbsp;cited</li>
</ul>
<p>The first item in that list is especially cool, because instead of using a standard
<span class="caps">RMSD</span> metric, they abstracted a  cluster-matching problem into a graph matching problem
and used an algorithm called Isorank to find an approximate greedy&nbsp;solution.</p>
<p>There is a solid greedy vs. dynamic explanation <a href="https://www.quora.com/Greedy-algorithm-vs-dynamic-programming-Whats-the-difference">here</a>.
The example I remember getting is that if you are a programmer for a <span class="caps">GPS</span>
direction provider, there are two main ways of deciding an optimal route.
At each gridpoint on a map, a greedy algorithm will take the fastest choice.
A dynamic algorithm will branch ahead, look into the future, and possibly
avoid short-term gain for long term drive-time savings. The greedy algorithm
might have a better best-case performance, but a much worse worse case&nbsp;performance.</p>
<p>In any case, we want to allow for the execution of a diffusion map algorithm where
a user can provide their own metric, tune the choice of epsilon, the choice of timescale,
and project the original trajectory <em>timesteps</em> onto the new dominant eigenvector, eigenvalue&nbsp;pairs.</p>
<h2 id="let-s-talk-api-actual-coding-hooray-">Let’s talk <span class="caps">API</span>/ Actual Coding&nbsp;(HOORAY!)</h2>
<p><code>DistMatrix</code></p>
<ul>
<li>Does frame by frame analysis on the trajectory, implements the <code>_prepare</code>
and <code>_single_frame</code> methods of the <code>BaseAnalysis</code> class</li>
<li>User selects a subset of a atoms in the trajectory&nbsp;here</li>
<li>This is where user provides their own metric, cutoff for when metric is equal,
weights for weighted metric calculation, and a start, stop, step for frame&nbsp;analysis</li>
</ul>
<p><code>Epsilon</code></p>
<ul>
<li>We will have some premade classes inheriting from epsilon, but all the <span class="caps">API</span>
will require is to return the manipulated <code>DistMatrix</code>, where each term has
now been divided by some scale parameter&nbsp;epsilon</li>
<li>These operations should be done in place on the original <code>DistMatrix</code>,
under no circumstances should we have two possibly large matrices sitting in&nbsp;memory</li>
</ul>
<p><code>DiffusionMap</code></p>
<ul>
<li>Accepts <code>DistMatrix</code> (initialized), <code>Epsilon</code> (uninitialized) with default a premade <code>EpsilonConstant</code>
class, timescale t with default = 1, weights of anisotropic kernel as&nbsp;parameters</li>
<li>Performs <code>BaseAnalysis</code> conclude method, wherein it exponentiates to the negative of
each term given by <code>Epsilon.scaledMatrix</code>, performs the procedure for the creation
of the anisotropic kernel above, and matrix multiplies anisotropic kernel by the timescale&nbsp;t.</li>
<li>Finally, eigenvalue decomposes the anisotropic kernel and holds onto the
eigenvectors and eigenvalues as&nbsp;attributes.</li>
<li>Should contain a method <code>DiffusionMap.embedding(timestep)</code>, that projects
a timestep to its diffusion embedding at the given timescale&nbsp;t.</li>
</ul>
<h2 id="jargon-box">Jargon&nbsp;Box</h2>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Crystallography">Crystallography</a></li>
<li><a href="https://en.wikipedia.org/wiki/Metric_(mathematics">Metric</a>)</li>
<li><a href="https://en.wikipedia.org/wiki/Configuration_space">Configuration&nbsp;Space</a></li>
<li><a href="https://en.wikipedia.org/wiki/Random_walk">Random&nbsp;Walk</a></li>
<li><a href="https://en.wikipedia.org/wiki/Markov_process">Markov&nbsp;Process</a></li>
<li><a href="https://en.wikipedia.org/wiki/Diffusion_process">Diffusion&nbsp;Process</a></li>
<li><a href="https://en.wikipedia.org/wiki/Fokker%E2%80%93Planck_equation">Fokker-Plank&nbsp;Equation</a></li>
<li><a href="https://en.wikipedia.org/wiki/Isotropy">Isotropic</a></li>
<li><a href="https://en.wikipedia.org/wiki/Anisotropy">Anisotropic</a></li>
<li><a href="https://en.wikipedia.org/wiki/Root-mean-square_deviation_of_atomic_positions"><span class="caps">RMSD</span></a></li>
<li><a href="https://groups.csail.mit.edu/cb/mna/">Isorank</a></li>
</ul>
<p>Works&nbsp;Cited:</p>
<ul>
<li><a href="http://dip.sun.ac.za/~herbst/research/publications/diff_maps_prasa2008.pdf">An Introduction to Diffusion&nbsp;Maps</a></li>
<li><a href="http://cis-linux1.temple.edu/~latecki/Courses/RobotFall08/Papers/DiffusionMaps06.pdf">Diffusion&nbsp;Maps</a></li>
<li><a href="http://www.ncbi.nlm.nih.gov/pubmed/20570730">Everything you wanted to know about Markov State Models but were afraid to&nbsp;ask</a></li>
<li><a href="http://papers.nips.cc/paper/2942-diffusion-maps-spectral-clustering-and-eigenfunctions-of-fokker-planck-operators.pdf">Diffusion Maps, Spectral Clustering and Eigenfunctions of Fokker-Planck&nbsp;Operators</a></li>
<li><a href="http://ferguson.matse.illinois.edu/resources/5.pdf">Nonlinear dimensionality reduction in molecular simulation: The diffusion map&nbsp;approach</a></li>
<li><a href="http://ferguson.matse.illinois.edu/resources/4.pdf">Integrating diffusion maps with umbrella sampling: Application to&nbsp;alanine</a></li>
<li><a href="http://ferguson.matse.illinois.edu/resources/2.pdf">Systematic determination of order parameters for chain dynamics using diffusion&nbsp;maps</a></li>
<li><a href="http://ferguson.matse.illinois.edu/resources/10.pdf">Nonlinear Machine Learning of Patchy Colloid Self-Assembly Pathways and&nbsp;Mechanisms</a></li>
<li><a href="https://www.cs.rice.edu/CS/Robotics/papers/das2006low-dim-land-folding-dim-reduction.pdf">Low-Dimensional Free Energy Landscapes of Protein Folding Reactions by Nonlinear Dimensionality&nbsp;Reduction</a></li>
<li><a href="http://www.ncbi.nlm.nih.gov/pubmed/21456654">Determination of reaction coordinates via locally scaled diffusion&nbsp;map</a></li>
<li><a href="http://scitation.aip.org/content/aip/journal/jcp/134/14/10.1063/1.3575245">Polymer reversal rate calculated via locally scaled diffusion&nbsp;map</a></li>
<li><a href="http://pubs.acs.org/doi/abs/10.1021/acs.jctc.6b00212">Machine Learning Based Dimensionality Reduction Facilitates Ligand Diffusion Paths Assessment: A Case of Cytochrome&nbsp;P450cam</a></li>
<li><a href="http://scitation.aip.org/content/aip/journal/jcp/141/11/10.1063/1.4893963">Diffusion maps, clustering and fuzzy Markov modeling in peptide folding&nbsp;transitions</a></li>
</ul>
</section>
        </article>
      </div>
    </div>
    <footer>
      <div class="content-wrap">
        <div class="nav"><a href="/">&laquo; Full blog</a></div>
        <section class="about"><p>Wintersmith is made by <a href="http://johan-nordberg.com">Johan Nordberg</a> and licensed under the <a href="http://opensource.org/licenses/MIT">MIT-license</a>.</p>

        </section>
        <section class="copy">
          <p>&copy; 2016 John Detlefs &mdash; powered by&nbsp;<a href="https://github.com/jnordberg/wintersmith">Wintersmith</a>
          </p>
        </section>
      </div>
    </footer>
  </body>
</html>