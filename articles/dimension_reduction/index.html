<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <meta name="viewport" content="width=device-width">
    <title>Dimension Reduction, a review of a review - John Detlefs' Web Log
    </title>
    <link rel="alternate" href="/feed.xml" type="application/rss+xml" title="a blog for my coding adventures">
    <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic|Anonymous+Pro:400,700,400italic,700italic|Merriweather:400,700,300">
    <link rel="stylesheet" href="/css/main.css">
  </head>
  <body class="article-detail">
    <header class="header">
      <div class="content-wrap">
        <h1>Dimension Reduction, a review of a review</h1>
        <p class="author">Written by <span class="author">John Detlefs</span>
        </p>
      </div>
    </header>
    <div id="content">
      <div class="content-wrap">
        <article class="article">
          <section class="content"><script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script>
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<p>Hello! This is my first post moving over to a new site built by
<a href="https://github.com/jnordberg/wintersmith">wintersmith</a>. Originally I was going
to use jekyll pages, but there was an issue with the latest ruby version not being
available for Linux, (maybe macs are better…). I spent <em>way too much</em> time
figuring out how to install a markdown plugin that allowed for the inclusion of
Latex. I did this all without realizing I could simply&nbsp;include:</p>
<pre><code>&lt;script type=&quot;text/javascript&quot; async
  src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;
&lt;/script&gt;
</code></pre><p>below my article title and latex would easily render. Now that this roadblock is
cleared, I have no excuses preventing me from writing a post about my&nbsp;work.  </p>
<p><span class="more"></span></p>
<p>This post is meant to discuss various dimension reduction methods in brief
as a preface to a more in-depth post about diffusion maps as they pertain
to chemistry. It assumes basic math skills, but will try to briefly explain
high-level concepts from Math and Stats. Towards the end I will provide a segue
into the next post, which will be posted on the same&nbsp;day.</p>
<p><a href="https://en.wikipedia.org/wiki/Dimensionality_reduction">Dimension reduction</a>
is performed on a data matrix $ M $ consisting of <em>n</em> ‘feature vectors’ wherein
each vector has a set of <em>m</em> data points associated with it. The data in the matrix
is considered to have dimension <em>m</em>, but oftentimes the actual ‘intrinsic dimensionality’
is much lower. As Laurens van der Maaten <a href="https://www.tilburguniversity.edu/upload/59afb3b8-21a5-4c78-8eb3-6510597382db_TR2009005.pdf">defines it</a>, ‘intrinsic dimensionality’
is ‘the the minimum number of parameters needed to account for the observed properties of the&nbsp;data’.</p>
<p>(So far, the most didactic explanation of this fact was presented
in a paper on diffusion maps by <a href="http://dip.sun.ac.za/~herbst/research/publications/diff_maps_prasa2008.pdf">Porte et al</a>
In the paper, a dataset of m-by-n pixel pictures of a simple image randomly rotated
originally has dimension (mn) but after dimension reduction, the dataset can be
organized two dimensionally based on angle-of&nbsp;rotation.)</p>
<p>At the most abstract level, dimension reduction methods usually are posed as an
optimization problem that ultimately requires the solution to an eigenvalue problem.
What is an <a href="https://en.wikipedia.org/wiki/Optimization_problem">optimization problem</a> you ask?
That wikipedia article should help some, the optimization being done in dimension
reduction is finding some linear or non-linear relation $ M $ that minmizes (or maximizes)
a cost function $ \phi (x) $ on some manipulation of the data matrix, call it $ X_{manipulated} $.
Examples of various functions will be given in detail&nbsp;later.</p>
<p>In most cases this can be turned into an eigenproblem posed as:
$$ X_{manipulated} M = \lambda M&nbsp;$$</p>
<p>In some cases, the manipulation is somewhat more complicated, and creates what
is called a <a href="https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix#Generalized_eigenvalue_problem"><em>generalized eigenvalue problem</em></a>.
In these situations the problem posed is $$ X_a M = \lambda X_b M $$
Where $X_a$ and $X_b$ are distinct but both still generated from some manipulation
on the original data matrix&nbsp;X.</p>
<p>The methods discussed so far necessitate the use of convex cost functions to be
optimized against. In set theory, convexity indicates that a set contains its subsets.
An open interval $ (a,b) $ is convex whereas the open interval minus a point is not.
Convex functions are similar but not entirely related, a convex function does not
have any <em>local optima</em> which means that if you’re at a maximum, you know it is&nbsp;global.</p>
<p>(I think there is a reason why people in optimization refer to surfaces as landscapes.
An interesting surface may have many hills and valleys, and finding an optimal path
is like a hiker trying to cross a mountain path blind, potentially&nbsp;problematic.)</p>
<p>Convex functions will always achieve the same solution given some input parameters,
but non-convex functions may get stuck on some local optima. This is why a method
like <a href="https://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf">t-<span class="caps">SNE</span></a> will converge to different results on different&nbsp;iterations.</p>
<p>It might be a little silly, but the rest of this post will be focused on providing
a quick explanation of various dimension reduction techniques. The general format
will be:
+optimization problem posed
+formal eigenvalue problem given
+interesting insights and relations
+pictures that I like from other&nbsp;work  </p>
<h2 id="works-cited">Works&nbsp;Cited</h2>
</section>
        </article>
      </div>
    </div>
    <footer>
      <div class="content-wrap">
        <div class="nav"><a href="/">&laquo; Full blog</a></div>
        <section class="about"><p>Wintersmith is made by <a href="http://johan-nordberg.com">Johan Nordberg</a> and licensed under the <a href="http://opensource.org/licenses/MIT">MIT-license</a>.</p>

        </section>
        <section class="copy">
          <p>&copy; 2016 John Detlefs &mdash; powered by&nbsp;<a href="https://github.com/jnordberg/wintersmith">Wintersmith</a>
          </p>
        </section>
      </div>
    </footer>
  </body>
</html>