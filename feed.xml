<?xml version="1.0" encoding="utf-8" ?>
<rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:wfw="http://wellformedweb.org/CommentAPI/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>John Detlefs' Web Log</title>
    <atom:link href="/feed.xml" rel="self" type="application/rss+xml"></atom:link>
    <link></link>
    <description>a blog for my coding adventures</description>
    <pubDate>Tue, 19 Jul 2016 17:00:00 -0700</pubDate>
    <generator>Wintersmith - https://github.com/jnordberg/wintersmith</generator>
    <language>en</language>
    <item>
      <title>Scipy 2016!</title>
      <link>/articles/scipy2016/</link>
      <pubDate>Tue, 19 Jul 2016 17:00:00 -0700</pubDate>
      <guid isPermaLink="true">/articles/scipy2016/</guid>
      <author></author>
      <description>&lt;p&gt;Last week I went to Austin, TX to Scipy2016. I wasn’t sure what to expect. How would people communicate?
Would I fit in, what talks would interest me? Fortunately the conference was a huge success.
I have came away a far more confident and motivated programmer than when I went in.&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;more&quot;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&quot;so-what-were-the-highlights-of-my-experience-at-scipy-&quot;&gt;So what were the highlights of my experience at Scipy?&lt;/h2&gt;
&lt;p&gt;On a personal level, I got to meet some of my coworkers, the members of the
Beckstein Lab. Dr. Oliver Beckstein, David Dotson, and Sean Seyler are brilliant
physicists and programmers who I have been working with on &lt;a href=&quot;/articles/scipy2016/www.mdanalysis.org&quot;&gt;MDAnalysis&lt;/a&gt;
and &lt;a href=&quot;/articles/scipy2016/www.datreant.org&quot;&gt;datreant&lt;/a&gt;. It was surreal to meet the people you have been
working with over the internet for 3 months and get an idea of how they
communicate and what they enjoy outside of work. It was the modern day
equivalent of meeting penpals for the first time. I especially appreciated
that David Dotson and Sean Seyler, both approximately four years my senior, provided
invaluable advice to a recent graduate. (If you’re reading this, thanks guys).&lt;/p&gt;
&lt;p&gt;The most valuable moments were the conversations I had in informal settings.
There is a huge diversity in career trajectories among those attending Scipy,
everyone has career advice and technical knowledge to impart upon a young
graduate as long as you are willing to ask. I had excellent conversations with
people from &lt;a href=&quot;/articles/scipy2016/cloverhealth.com&quot;&gt;Clover Health&lt;/a&gt;, Apple data scientists, Andreas
Klockner (Keynote Speaker), Brian Van de Ven (Bokeh Dev), Ana Ruvalcaba at
&lt;a href=&quot;/articles/scipy2016/jupyter.org&quot;&gt;Jupyter&lt;/a&gt;, the list goes on…&lt;/p&gt;
&lt;h2 id=&quot;fascinating-troubling-and-unexpected-insights&quot;&gt;Fascinating, Troubling, and Unexpected Insights&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Scipy doubled in size in the last year!&lt;/li&gt;
&lt;li&gt;So many free shirts (and stickers), don’t even bother coming with more than one shirt,
also nobody wears professional attire.&lt;/li&gt;
&lt;li&gt;Overheard some troubling comments made by men at Scipy, e.g.
“Well, all the women are getting the jobs I’m applying for…”
(said in a hallway group, this is not appropriate even if it was a joke)&lt;/li&gt;
&lt;li&gt;The amount of beer involved in social events is kind of nuts;
this probably comes with the territory of professional programming.&lt;/li&gt;
&lt;li&gt;There are a lot of apologists for rude people, someone can be extremely
nonverbally dismissive and when you bring it up to other
people they will defend him (yes, always him) saying something to the effect of
‘he has been really busy recently’. Oliver Beckstein is a shining example of
someone who is very busy and makes a conscious effort to always be thoughtful
and kind.&lt;/li&gt;
&lt;li&gt;Open source does not always imply open contribution, some companies represented at
Scipy maintain open source projects while making the barriers to contribution prohibitively high.&lt;/li&gt;
&lt;li&gt;A lot of people at Scipy apologize for their job (half-seriously) if they aren’t
someone super-special like a matplotlib core developer or the inventor of Python.
Your jobs are awesome people!&lt;/li&gt;
&lt;li&gt;It is &lt;em&gt;really&lt;/em&gt; hot in Austin.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;git pull&lt;/code&gt; is just &lt;code&gt;git fetch&lt;/code&gt; + &lt;code&gt;git merge&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;A lot of women in computing have joined and left male dominated organizations
not because people are necessarily mean, but because they’ve been asked out
too much or harassed in a similar fashion. Stay professional folks.&lt;/li&gt;
&lt;li&gt;Cows turn inedible corn into edible steak.&lt;/li&gt;
&lt;li&gt;As a young professional you have to work harder and take every moment more seriously than those older than you
in order to get ahead.&lt;/li&gt;
&lt;li&gt;Breakfast tacos are delicious.&lt;/li&gt;
&lt;li&gt;Being able to get out of your comfort zone is a professional asset.&lt;/li&gt;
&lt;li&gt;Slow down, take a breath, read things over, don’t make simple mistakes.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;here-are-some-talks-i-really-enjoyed&quot;&gt;Here are some talks I really enjoyed&lt;/h2&gt;
&lt;h3 id=&quot;datashader-&quot;&gt;Datashader!&lt;/h3&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/6m3CFbKmK_c&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt;

&lt;h3 id=&quot;dating-&quot;&gt;Dating!&lt;/h3&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/dtgmMj8W298&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt;

&lt;h3 id=&quot;loo-py-&quot;&gt;Loo.py!&lt;/h3&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/Zz_6P5qAJck&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt;

&lt;h3 id=&quot;dask-&quot;&gt;Dask!&lt;/h3&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/PAGjm4BMKlk&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt;
</description>
    </item>
    <item>
      <title>Principal Component Analysis</title>
      <link>/articles/pca/</link>
      <pubDate>Tue, 28 Jun 2016 17:00:00 -0700</pubDate>
      <guid isPermaLink="true">/articles/pca/</guid>
      <author></author>
      <description>&lt;script type=&quot;text/x-mathjax-config&quot;&gt;
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot; async
  src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;
&lt;/script&gt;

&lt;p&gt;My next subject for bloggery is Principal Component Analysis (PCA)
(its sibling Multidimensional scaling has been left out for a future post,
but it is just as special, don’t worry). If I were to give a talk on PCA,
the slides would be roughly ordered as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A very short recap of dimension reduction&lt;/li&gt;
&lt;li&gt;PCA, what it stands for, rough background, history&lt;/li&gt;
&lt;li&gt;Eigenvectors (what are those?!)&lt;/li&gt;
&lt;li&gt;Covariance (Because variance matrix didn’t sound cool enough)&lt;/li&gt;
&lt;li&gt;The very fancy sounding method of Lagrange Multipliers (why they aren’t that hard)&lt;/li&gt;
&lt;li&gt;Explain the PCA Algorithm&lt;/li&gt;
&lt;li&gt;Random Walks: What are they, how are they taken on a configuration space&lt;/li&gt;
&lt;li&gt;Interpreting the results after applying PCA on MD simulation data&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&quot;more&quot;&gt;&lt;/span&gt;
In reality not going to follow these bullet points, if you want to get information
pertaining to the first two points, please read some of my previous posts.
The last two points are going to be a subject for a post next week.&lt;/p&gt;
&lt;p&gt;Here are some good sources for those seeking to acquaint themselves with
Linear Algebra and Statistics. &lt;a href=&quot;https://onlinecourses.science.psu.edu/stat505/node/2&quot;&gt;Multivariate Statistics and PCA&lt;/a&gt;
(Lessons two through 10) and the &lt;a href=&quot;http://www.feynmanlectures.caltech.edu/I_06.html&quot;&gt;Feynman Lecture on Physics: Probability&lt;/a&gt;.
The Feynman lectures on physics are &lt;em&gt;so&lt;/em&gt; good and &lt;em&gt;so&lt;/em&gt; accessible. Richard
Feynman certainly had his flaws but teaching was not one of them. If you’re
too busy to read those, here’s a quick summary of some important ideas I
will be using.&lt;/p&gt;
&lt;h2 id=&quot;what-is-a-linear-transformation-john-&quot;&gt;What is a Linear Transformation, John?&lt;/h2&gt;
&lt;p&gt;Glad you asked, friend! Let’s just stick to linearity.
for a function to be linear, it means that $f(a+b) = f(a) + f(b)$. As an
example of a non-linear function, consider $ y = x^{3} $ . After plugging
some numbers in we can see this is non-linear $ 2^{3} \neq 1^{3} + 1^{3} $.&lt;/p&gt;
&lt;p&gt;A transformation at its most abstract is the description of an algorithm
that gets an object A to become an object B.
In linear algebra the transformation is being done on vectors belonging to a
domain (where vectors exist before the transformation) space  $V$ and a range
(where vectors exist after the transformation) space $W$. For the purposes of
our work, these are both $R^{n}$, the Cartesian product n-times of the real line.
(The standard (x,y) coordinate system is the Cartesian product of the real line
twice, or $R^{2}$)&lt;/p&gt;
&lt;p&gt;When dealing with vector spaces, our linear transformation can be represented
by a $m$-by-$n$ matrix, where $n$ is the dimension of the space we are sending a
vector into (always less than or equal to m), and $m$ is the dimension of the
vector space in which our original vector (or set of vectors) being transformed
originally exists in. So if we have some set of $k$ vectors being transformed,
the matrices will be have row-by-column sizes:
$$[ k-by-m ] [m-by-n] = [k-by-n]$$. These maps can be scalings, rotations,
shearings and more.&lt;/p&gt;
&lt;h2 id=&quot;what-is-a-vector-what-is-an-eigenvector-&quot;&gt;What is a vector, what is an eigenvector?&lt;/h2&gt;
&lt;p&gt;Good question! A vector has a magnitude (which is just some positive number
for anything that we are doing) and a direction (a property that is drawn
from the &lt;em&gt;vector space&lt;/em&gt; to which the vector belongs). Being told to walk 10
paces due north is to follow a vector with magnitude 10
and direction north. Vectors are presented as if they are centered at the origin,
and their &lt;em&gt;head&lt;/em&gt; is reflects their magnitude and direction. This allows some
consistency when discussing things, when we are given the vector $(1,1)$ in $R^2$
we know it is centered at the origin, and thus has magnitude (from the distance
formula) of $\sqrt{2}$ and the direction is 45 degrees from the horizontal
axis. An &lt;strong&gt;eigenvector&lt;/strong&gt; sounds a lot scarier than it is;
the purpose of an eigenvector is to answer the question,
‘what vectors don’t change direction under a given linear transformation?’
&lt;img src=&quot;/articles/pca/Mona_Lisa_eigenvector_grid.png&quot; alt=&quot;mona lisa eigenvector&quot;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;This picture is stolen from wikipedia, but it should be clear that the
blue vector is an eigenvector of this transformation, while the red vector
is not.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The standard equation given when an eigenvalue problem is posed is:
$$ Mv = \lambda v $$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;$M$ is some linear transformation, $v$ is an eigenvector
we are trying to find, and $\lambda$ is the corresponding eigenvalue.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;From this equation, we can see that eigenvector-eigenvalue pairs
are not unique; direction is not a unique property of a vector. If we
find a vector $v$ that satisfies this equation for our linear
transformation $M$, scaling the vector by some constant $\alpha$
will simply change the eigenvalue associated with the solution.
The vector $(1,1)$ in $R^2$ has the same direction as
$(2,2)$ in $R^2$. If one of these vectors isn’t subject to a direction change
(and therefore an eigenvector), then the other must be as well, because the
eigenvector-ness (yes, I just coined this phrase) applies to all vectors with
the same direction.&lt;/p&gt;
&lt;p&gt;For those of you more familiar with Linear Algebra, this
should not be confused with the fact that a linear transformation can have
degenerate eigenvalues. This concept of degenerate eigenvalues comes up when
the rank of the matrix representation of a linear transformation is less than
its dimension, but given that our transformation has rank equal to to the
dimension of the vector space, we can ignore this.&lt;/p&gt;
&lt;h2 id=&quot;statistics-in-300-words&quot;&gt;Statistics in ~300 words&lt;/h2&gt;
&lt;p&gt;Sticking  one dimension, plenty of data seems to be random while also favoring
a central point. Consider the usual example of height across a population of
people. Height can be thought of as a &lt;em&gt;random variable&lt;/em&gt;. But this isn’t random
in the way that people might think about randomness without some knowledge of
statistics. There is a central point where the heights of people tend to cluster,
and the likelihood of someone being taller or shorter than this central point
decreases on a ‘bell-curve’.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/articles/pca/univar_gaussian.svg&quot; alt=&quot;normal&quot;&gt;&lt;/p&gt;
&lt;p&gt;This is called a &lt;em&gt;normal distribution&lt;/em&gt;. Many datasets can be thought of as
describing the a set of outcomes for some random variable in nature.
These outcomes are distributed in some fashion. In our example, the &lt;em&gt;mean&lt;/em&gt; of
the data is the average height over the entire population. The &lt;em&gt;variance&lt;/em&gt; of
our data is how far the height of a  is spread out from its mean. When the
distribution of outcomes follows a bell-curve such as it does in our example,
the distribution is referred to as &lt;em&gt;normal&lt;/em&gt;. (There are some more
technical details, but the phrase normal stems from the fact that total area
under the bell-curve defined by the normal distribution is equal to one.) When
the data we want to describe is reflects more than one random variable this
is a &lt;em&gt;multivariate distribution&lt;/em&gt;.
&lt;img src=&quot;/articles/pca/multivar_gaussian.svg&quot; alt=&quot;multivariate&quot;&gt;
Statistics introduces the concept of &lt;em&gt;covariance&lt;/em&gt; to describe the relationship
that random variables have with one another. The magnitude of a covariance indicates in some fashion the
relationship between random variables; it is not easily interpretable without
some set of constraints on the covariances, which will come up in
Principal Component Analysis. The sign of the covariance between two random
variables (X,Y) indicates if the two points are inversely related or directly
related.  A negative covariance between X and Y means that increasing X decreases
while a positive covariance means that increasing X increases Y. The normalized
version of covariance is called the &lt;em&gt;correlation coefficient&lt;/em&gt; and might be
more familiar to those previously acquainted with statistics.&lt;/p&gt;
&lt;h2 id=&quot;constrained-optimization&quot;&gt;Constrained Optimization&lt;/h2&gt;
&lt;p&gt;Again, please do look at the Feynman Lectures, if you’re unfamiliar
with statistics look at the &lt;a href=&quot;https://onlinecourses.science.psu.edu/stat505/node/2&quot;&gt;Penn State&lt;/a&gt;
material for the ideas I just went over to better understand them.
The last subject I want to broach before getting into the details of PCA is
optimization with &lt;a href=&quot;https://www.youtube.com/watch?v=nDuS5uQ7-lo&quot;&gt;Lagrange Multipliers&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Lagrange Multipliers are one method of solving a &lt;em&gt;constrained optimization problem&lt;/em&gt;.
Such a problem requires an objective function to be optimized and constraints
to optimize against. An objective function is any function that we wish to
maximize or minimize to reflect some target quantity achieving an optimum value.
In short, the method of Lagrange Multipliers creates a system of linear equations
such that we can solve for a term $\lambda$ that shows when the objective
function achieves a maximum subject to constraints. In the case the of PCA, the
function we want to maximize is $M^{T} cov(X) M$, describing the
covariance of our data matrix $X$.  &lt;/p&gt;
&lt;h2 id=&quot;pca&quot;&gt;PCA&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/articles/pca/PCA-vectors.svg&quot; alt=&quot;PCA-vectors&quot;&gt;&lt;/p&gt;
&lt;p&gt;Although it is not quite a fortuitous circumstance, the principal components of
the covariance matrix are precisely it’s eigenvectors. For a multivariate dataset
generated by $n$ random variables, PCA will return a sequence of $n$ eigenvectors each
describing more covariance in the dataset than the next. The picture above
presents an example of the eigenvectors reflecting the covariance of a 2-dimensional
multivariate dataset.&lt;/p&gt;
&lt;p&gt;To be perfectly honest, I don’t know a satisfying way to explain why the eigenvectors
are the principal components. The best explanation I can come up with is that
the algorithm for PCA is in correspondence with the algorithm for eigenvalue
decomposition. It’s one of those things where I &lt;em&gt;should&lt;/em&gt; be able to provide a
proof for why the two problems are the same, but I cannot at the moment. (Commence
hand-waving…)&lt;/p&gt;
&lt;p&gt;Let’s look at the algorithm for Principal Component Analysis to better understand
things. PCA is an iterative method that seeks to create a sequence
of vectors that describe the covariance in a collection of data, each vector describing
more than those that will follow. This introduces an optimization problem, an
issue that I referenced earlier. In order to guarantee uniqueness of our solution,
this optimization is subject to constraints using Lagrange Multipliers.
remember, the video provides an example of a constrained optimization problem
from calculus. Principal Component Analysis finds this set of vectors by creating a linear
transformation $M$ that maximizes the covariance objective function given below.&lt;/p&gt;
&lt;p&gt;PCA’s objective function is  $trace(M^{T} cov(X) M)$, we are looking to maximize this.
From the Penn State lectures:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Earlier in the course we defined the total variation of X as the trace of the
variance-covariance matrix, or if you like, the sum of the variances of the
individual variables. This is also equal to the sum of the eigenvalues.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In the first step of PCA, we can think of our objective function as:
$$a^T cov(X) a$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;$a$ in this case is a single vector&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We seek to maximize this a term such that the sum of the squares of the coefficients
of a is equal to one. (In math terms this is saying that the
$L^2$ norm is one). This constraint is introduced to ensure that a unique answer
is obtained, (remember eigenvectors are not unique, this is the same process one
would undertake to get a unique sequence of eigenvectors from a decomposition!).
In the second step, the objective function is :
$$B^T cov(X) B$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;$B$ consists of $a$ and a new vector $b$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We look to maximize $b$ such that it explains covariance &lt;em&gt;not&lt;/em&gt;
previously explained by the first, for this reason we introduce an
optimization problem with two constraints:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$a$ remains the same&lt;/li&gt;
&lt;li&gt;The sum of the squares of the coefficients of $b$ equals one&lt;/li&gt;
&lt;li&gt;None of the covariance explained by vector $a$ is explained by $b$, (the two vectors are &lt;em&gt;orthogonal&lt;/em&gt;, another property of eigenvectors!)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This process is repeated for the entire transformation $M$. This gives us a sequence
of eigenvalues that each reflect some fraction of the covariance, and sum to one.
For our previously mentioned n-dimensional multivariate dataset, generally some
number $k \lt n$ of eigenvectors explain the total variance well enough to ignore
the $n - k$ vectors remaining. This gives a set of principal components to investigate.  &lt;/p&gt;
&lt;p&gt;It might follow that this corresponds to an eigenvalue problem:
$$cov(X)M = \lamba M$$
If it doesn’t appear to be clear, let’s step back and
look again at eigenvectors. As I said earlier, eigenvectors provide insight
into what vectors don’t change direction under a linear map. We are trying find
a linearly independent set of vectors that provide insight into the structure
of our covariance from our multivariate distribution. Eigenvectors are either
the same or orthogonal. This algorithm we described, is &lt;em&gt;precisely&lt;/em&gt; the
algorithm one would use to find the eigenvectors of any (full-rank) linear transformation,
not just the linear transformation done in Principal Component Analysis.&lt;/p&gt;
&lt;h2 id=&quot;chemistry-application-and-interpretation-&quot;&gt;Chemistry application and interpretation:&lt;/h2&gt;
&lt;p&gt;Before PCA is done on an MD Simulation, we have to consider what goals we
have for the analysis of results. We are searching to arrange the data
outputted by PCA such it gives us intuition into some physical behavior of our
system. This is usually done a single structure, and in order to focus insight
on relative structural change rather than some sort of translational motion of
the entire structure, an alignment of an entire trajectory to some structure of
interest by minimization of the Root Mean Square Distance must be done prior to
analysis. After RMS alignment, any variance in data should be variance due to
changes in structure.  &lt;/p&gt;
&lt;p&gt;Cecilia Clementi has this quote in her Free Energy Landscapes paper:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Essentially, PCA computes a hyperplane that passes through the data points as
best as possible in a least-squares sense. The principal components are the
tangent vectors that describe this hyperplane&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;How do we interpret n-dimensional physical data from the tangent vectors of an
k-dimensional hyperplane embedded in this higher dimensional space? This is all
very mathematical and abstract, . What we do is reduce the analysis to some
visually interpretable subset of components, and see if there is any indication
of clustering that occurs.  &lt;/p&gt;
&lt;p&gt;Remember, we have an explicit linear map relating the higher dimensional
space to the lower-dimensional space . By taking our trajectory and projecting
it onto one of the set of eigenvector components of our analysis, we can
extract embeddings in different ways, from Clementi again:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;So, the first principal component corresponds to the best possible projection onto
a line, the first two correspond to the best possible projection onto a plane, and so on.
Clearly, if the manifold of interest is inherently non-linear the low-dimensional e
mbedding obtained by means of PCA is severely distorted… The fact that empirical
reaction coordinates routinely used in protein folding studies can not be
reduced to a linear combination of the Cartesian coordinates underscores the
inadequacy of linear dimensionality reduction techniques to characterize a
folding landscape.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Again, this is all esoteric for the lay reader. What is a manifold, a reaction
coordinate, a linear combination of cartesian coordinates? All we should know is
that PCA is a limited investigational tool for complex systems, the variance the
principal components explain should not necessarily be interpreted as physical
parameters governing the behavior of a system.&lt;/p&gt;
&lt;p&gt;My mentor max has a great Jupyter notebook up demonstrating PCA done on MD
simulations &lt;a href=&quot;https://gist.github.com/kain88-de/0bfe0813e27ad601004b247fedb2ee7d&quot;&gt;here&lt;/a&gt;.
All of these topics are covered in the notebook and should be relatively accessible
if you understand what I’ve said so far. In my next post I will write about how
I will be implementing PCA as a module in MDAnalysis.&lt;/p&gt;
&lt;p&gt;-John&lt;/p&gt;
</description>
    </item>
    <item>
      <title>A Note from the Author</title>
      <link>/articles/note/</link>
      <pubDate>Mon, 27 Jun 2016 17:00:00 -0700</pubDate>
      <guid isPermaLink="true">/articles/note/</guid>
      <author></author>
      <description>&lt;p&gt;In the last blog post I wrote the most common critique I received was that
I alienated myself from most of my potential audience. In an email I expressed
to my Summer of Code mentor &lt;a href=&quot;http://www.biophys.mpg.de/en/hummer.html&quot;&gt;Max Linke&lt;/a&gt;
my problem:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;My number one worry in all of these matters is coming off as unrigorous or
pseudo-scientific, and I think I probably overcompensate by being borderline
inaccessible. I think this stems from some time spent enjoying a lack of rigor
and the fun that is being pseudo-scientific.  &lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;span class=&quot;more&quot;&gt;&lt;/span&gt;
&lt;a href=&quot;https://www.youtube.com/watch?v=n0-jKmcNr_8&quot;&gt;Cue Bill Nye making fun of Neil deGrasse Tyson for being full of it&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For a while, after perusing blogs and social media, I thought, “Hey, I strongly
identify with this ‘impostor syndrome’ thing.” Now I realize that’s a pretty ignorant
and borderline insulting view to have. From what I can tell, I may have insecurities,
but the difference between my anxieties and people with who struggle with true
‘impostor syndrome’ is that someone has to have experience tangible evidence that
they are an outsider. As a straight white male, I don’t have these problems.
So I guess in the future I will refrain from letting these anxieties be confused
with something more serious — I have it pretty easy.&lt;/p&gt;
&lt;p&gt;Going further, as a student and a tutor I noticed that far too often when people
were in over their heads, they would get quiet and close off to the outside
world. Especially in my math classes; a professor could be explaining Jordan
Normal Forms, reciting proofs and corollaries and lemmas as if they were
gospel, and although everyone was baffled, they would stay quiet. Nobody likes
it when someone dominates a lecture with their own questions and at the same
time a lot of people have missed fundamentals out of fear of sounding stupid.
If I’m ever asked in a job interview to give a personal strength, it would be
that I ask questions that might seem stupid with reckless abandon.
&lt;img src=&quot;/articles/note/mayonnaise.jpg&quot; alt=&quot;me in class&quot;&gt;&lt;/p&gt;
&lt;p&gt;These posts are intended for people working to teach
themselves a some difficult topics. I apologize for being obtuse and abstract
and abstruse earlier. I will do my best to  teach things from an
intuition-first standpoint from here on and provide resources for refreshing
on math and statistics topics. &lt;em&gt;Please&lt;/em&gt; get in touch with me if something I say
is unclear or wrong; this blog is as much for my own education as it is others.&lt;/p&gt;
&lt;p&gt;-John&lt;/p&gt;
</description>
    </item>
    <item>
      <title>Diffusion Maps in Molecular Dynamics Analysis</title>
      <link>/articles/diffusion_maps/</link>
      <pubDate>Tue, 14 Jun 2016 17:00:00 -0700</pubDate>
      <guid isPermaLink="true">/articles/diffusion_maps/</guid>
      <author></author>
      <description>&lt;script type=&quot;text/x-mathjax-config&quot;&gt;
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot; async
  src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;
&lt;/script&gt;

&lt;p&gt;It occurs to me in my previous post I didn’t thoroughly explain the motivation
for dimension reduction in general. When we have this data matrix $X$ with $n$
samples and each sample having $m$ features, this number m can be very large.
This data contains information that we want to extract, in the case of molecular
dynamics simulations these are parameters describing how the dynamics are occurring.
But this data can be features that distinguish faces from others in the dataset,
handwritten letters and numbers from other numbers, etc. As it is so eloquently put
by &lt;a href=&quot;http://dip.sun.ac.za/~herbst/research/publications/diff_maps_prasa2008.pdf&quot;&gt;Porte and Herbst at Arizona&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The breakdown of common similarity measures hampers
the efficient organisation of data, which, in turn, has
serious implications in the field of pattern recognition.
For example, consider a collection of n × m images,
each encoding a digit between 0 and 9. Furthermore, the
images differ in their orientation, as shown in Fig.1. A
human, faced with the task of organising such images,
would likely first notice the different digits, and thereafter
that they are oriented. The observer intuitively attaches
greater value to parameters that encode larger variances in the observations,
and therefore clusters the data in 10 groups, one for each digit&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Here we’ve been introduced to the idea of pattern recognition and ‘clustering’,
the latter will be discussed in some detail later. Continuing on…&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;On the other hand, a computer sees each image as a
data point in $R^{nm}$, an nm-dimensional coordinate space.
The data points are, by nature, organised according to
their position in the coordinate space, where the most
common similarity measure is the Euclidean distance.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The idea of the data being in a $nm$ dimensional space is introduced by the
authors. The important part is that a computer has no knowledge of the the patterns
inside this data. The human brain is excellent at plenty of algorithms, but dimension
reduction is one it is especially good at.&lt;/p&gt;
&lt;h2 id=&quot;start-talking-about-some-chemistry-john-&quot;&gt;Start talking about some chemistry John!&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;Fine! Back to the matter at hand, dimension reduction is an invaluable tool in
modern computational chemistry because of the massive dimensionality of molecular
dynamics simulations. To my knowledge, the biggest things being studied by MD
currently are on the scale of the &lt;a href=&quot;http://www.bio-itworld.com/2013/5/29/researchers-characterize-chemical-structure-hiv-capsid.html&quot;&gt;HIV-1 Capsid&lt;/a&gt; at 64 million atoms!
Of course, these studies are being done on supercomputers, and for the most part
studies are running on a much smaller number of atoms. For a thorough explanation
of how MD simulations work, my Summer of Code colleague &lt;a href=&quot;http://fiona-naughton.github.io/blog/2016/05/25/What-is-this-MD-thing-anyway&quot;&gt;Fiona Naughton&lt;/a&gt;
has an excellent and cat-filled post explaining MD and Umbrella Sampling. Why do we
care about dynamics? As &lt;a href=&quot;https://chemistry.rice.edu/FacultyDetail.aspx?RiceID=238&quot;&gt;Dr. Cecilia Clementi&lt;/a&gt; mentions in her &lt;a href=&quot;http://cgl.uni-jena.de/pub/Workshops/WebHome/CGL_workshop1.pdf&quot;&gt;slides&lt;/a&gt;,
‘Crystallography gives structures’, but function requires dynamics!’&lt;/p&gt;
&lt;p&gt;A molecular dynamics simulation can be thought of as a diffusion process subject
to drag (from the interactions of molecules) and random forces, (brownian motion).
This means that the time evolution of the probability density of a molecule occupying
a point in the configuration space $P(x,t)$ satisfies the &lt;a href=&quot;https://en.wikipedia.org/wiki/Fokker%E2%80%93Planck_equation&quot;&gt;Fokker-Plank Equation&lt;/a&gt;
(This is some complex math from statistical mechanics). The important thing to note
is that the Fokker-Plank equation has a discrete eigenspectrum, and that there
usually exists a spectral gap reflecting the ‘intrinsic dimensionality’ of the
system it is modeling. A diffusion process is by definition markovian, in this case a continuous
markov process, which means the state at time t is solely dependent on the instantaneous
step before it. This is easier when transferred over to the actual discrete problems
in MD simulation, the state at time $t$ is only determined by the state at time $t-1$.&lt;/p&gt;
&lt;p&gt;Diffusion maps in MD try to find a discrete approximation of the
eigenspectrum of the Fokker-Plank equation by taking the following steps. First, we can think of
changes in configuration as random walks on an infinite graph defined by the
configuration space. &lt;img src=&quot;/articles/diffusion_maps/WalksOnGraph.png&quot; alt=&quot;graph&quot;&gt; From &lt;a href=&quot;http://dip.sun.ac.za/~herbst/research/publications/diff_maps_prasa2008.pdf&quot;&gt;Porte&lt;/a&gt; again:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The connectivity between two data points, x and y, is
defined as the probability of jumping from x to y in one
step of the random walk, and is&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;$$ connectivity(x,y) = p(x,y) $$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt; It is useful to express this connectivity in terms of a non-normalised
likelihood function, k, known as the diffusion kernel:&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;$$ connectivity  \propto  k(x,y) $$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The kernel defines a local measure of similarity within
a certain neighbourhood. Outside the neighbourhood, the
function quickly goes to zero. For example, consider the
popular Gaussian kernel:&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;$$ k(x,y) = \exp(-\frac{|x-y|^{2}}{\epsilon}) $$&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://cis-linux1.temple.edu/~latecki/Courses/RobotFall08/Papers/DiffusionMaps06.pdf&quot;&gt;Coifman and Lafon&lt;/a&gt; provide a dense but extremely thorough explanation of diffusion maps in their seminal paper. This quote screams molecular dynamics:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Now, since the sampling of the data is generally not related to the geometry of the manifold, one would like to recover the manifold structure regardless of the distribution of the data points. In the case when the data points are sampled from the equilibrium distribution of a stochastic dynamical system, the situation is quite different as the density of the points is a quantity of interest, and therefore, cannot be gotten rid of. Indeed, for some dynamical physical systems, regions of high density correspond to minima of the free energy of the system. Consequently, the long-time behavior of the dynamics of this system results in a subtle interaction between the statistics (density) and the geometry of the data set.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In this paper, the authors acknowledge that oftentimes an isotropic kernel is not sufficient to understand the relationships in the data. He poses the question:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In particular, what is the influence of the density of the points and of the geometry of the possible underlying data set over the eigenfunctions and spectrum of the diffusion?
 To address this type of question, we now introduce a family of anisotropic diffusion processes that are all obtained as small-scale limits of a graph Laplacian jump process. This family is parameterized by a number $\alpha$ which can be tuned up to specify the amount of influence of the density in the infinitesimal transitions of the diffusion. The crucial point is that the graph Laplacian normalization is not applied on a &amp;gt;graph with isotropic weights, but rather on a renormalized graph.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The derivation from here requires a few more steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Form a new kernel from anisotropic diffusion term: Let $$ q_{\epsilon}(x) = \int k{\epsilon}(x,y)q(y) \,dy$$&lt;br&gt;Where
$$ k{\epsilon}^{(\alpha)} =  \frac{k{\epsilon}(x,y)}{q{\epsilon}(x) q{\epsilon}(y) }$$&lt;/li&gt;
&lt;li&gt;Apply weighted graph Laplacian normalization:
$$ d{\epsilon}^{(\alpha)}(x) = \int k{\epsilon}^{(\alpha)}(x,y)q(y) \,dy $$&lt;/li&gt;
&lt;li&gt;Define anisotropic transition kernel from this term
$$ p{\epsilon,\alpha}(x, y) =   \frac{k{\epsilon}^{(\alpha)}(x,y)}{d_{\epsilon}^{(\alpha)}(x)}$$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This was all kinds of painful, but what this means for diffusion maps in MD is
that a meaningful diffusion map will have an anisotropic, (and therefore unsymmetric kernel).
&lt;a href=&quot;http://cis-linux1.temple.edu/~latecki/Courses/RobotFall08/Papers/DiffusionMaps06.pdf&quot;&gt;Coifman and Lafon&lt;/a&gt; go on to prove that for $\alpha$ equal to $\frac{1}{2}$
this anisotropic kernel is an effective approximation for the Fokker-Plank equation!
This is a really cool result that is in no way obvious.&lt;/p&gt;
&lt;p&gt;Originally, when I studied diffusion maps while applying for the Summer of Code
I was completely unaware of Fokker-Plank and the anisotropic kernel. Of course,
learning these topics takes time, but I was under the impression that diffusion
kernels were symmetric across the board, which is just dead wrong. This of course
changes how eigenvalue decomposition can be performed on a matrix and requires a
routine like Singular Value Decomposition instead of Symmetric Eigenvalue Decomposition.
If I had spent more time researching literature on my own I think I could have figured this out.
With that being said, there are 100+ dense pages given in the citations below.&lt;/p&gt;
&lt;p&gt;So where are we at? Quick recap about diffusion maps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Start taking random walks on a graph&lt;/li&gt;
&lt;li&gt;There are different costs for different walks based on likelihood of walk happening&lt;/li&gt;
&lt;li&gt;We established a kernel based on all these different walks&lt;/li&gt;
&lt;li&gt;For MD we manipulate this kernel so it is anisotropic!&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Okay, so what do we have left to talk about…&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How is epsilon determined?&lt;/li&gt;
&lt;li&gt;What if we want to take a random walk of more than one jump?&lt;/li&gt;
&lt;li&gt;Hey John, we’re not actually taking random walks!&lt;/li&gt;
&lt;li&gt;What do we do once we get an eigenspectrum?&lt;/li&gt;
&lt;li&gt;What do we use this for?&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;epsilon-determination&quot;&gt;Epsilon Determination&lt;/h2&gt;
&lt;p&gt;Epsilon determination is kind of funky. First off, &lt;a href=&quot;http://ferguson.matse.illinois.edu/page1/&quot;&gt;Dr. Andrew L. Ferguson&lt;/a&gt; notes
that division by epsilon retrains ‘only short pairwise distances on the order of $\sqrt{2\epsilon}$’.
In addition, &lt;a href=&quot;https://chemistry.rice.edu/FacultyDetail.aspx?RiceID=238&quot;&gt;Dr. Clementi&lt;/a&gt; in her &lt;a href=&quot;http://cgl.uni-jena.de/pub/Workshops/WebHome/CGL_workshop1.pdf&quot;&gt;slides&lt;/a&gt; on diffusion maps notes that the
neighborhood determined by epsilon should be locally flat. For a free-energy surface, this means that it
is potentially advantageous to define a unique epsilon for every single element of a kernel based on the
nearest neighbors to that point in terms of value. This can get painful.
Most researchers seem to use constant epsilon determined from some sort of guess and check method based on clustering.&lt;/p&gt;
&lt;p&gt;For my GSoC pull request that is up right now, the plan is to have an API
for an &lt;code&gt;Epsilon&lt;/code&gt; class that must return a matrix whose $ij th$ coordinate is $\frac{d(i,j)^2}{\epsilon_ij }$.
From here, given weights for the anisotropy of the kernel, we can form the anisotropic kernel
to be eigenvalue-decomposed. Any researcher who cares to do some complex choice
of epsilon based on nearest-neighbors is probably a good enough hacker to handle implementation of this API in a quick script.&lt;/p&gt;
&lt;h2 id=&quot;length-t-walks&quot;&gt;Length $t$ Walks&lt;/h2&gt;
&lt;p&gt;Nowhere in the construction of our diffusion kernel are we actually taking random walks.
What we are doing is taking all possible walks, where two vertices on the graph are close if $d(x,y)$ is small
and far apart if $d(x,y)$ is large. This accounts for all possible one-step walks across our data.
In order to get a good idea of transitions that occur over larger timesteps, we take multiple steps.
To construct this set of walks, we must multiply our distance matrix $P$ by itself t-times,
where t is the number of steps in the walk across the graph.
From Porte again (stealing is the best form of flattery, no?):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;With increased values of t (i.e. as the diffusion process
“runs forward”), the probability of following a path
along the underlying geometric structure of the data set
increases. This happens because, along the geometric
structure, points are dense and therefore highly connected
(the connectivity is a function of the Euclidean distance
between two points, as discussed in Section 2). Pathways
form along short, high probability jumps. On the other
hand, paths that do not follow this structure include one
or more long, low probability jumps, which lowers the
path’s overall probability.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I said something blatantly wrong in my last post. I’m a fool, but still, things do
get a little complicated when analyzing time series data with diffusion maps.
We want to both investigate different &lt;em&gt;timescale&lt;/em&gt; walks from the diffusion maps,
but also to be able to project our snapshot from a trajectory at a &lt;em&gt;timestep&lt;/em&gt;
to the corresponding set of eigenvectors describing the lower dimensional order-parameters.&lt;/p&gt;
&lt;p&gt;From &lt;a href=&quot;http://ferguson.matse.illinois.edu/page1/&quot;&gt;Ferguson&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The &lt;strong&gt;diffusion map embedding&lt;/strong&gt; is defined as the mapping of the
ith snapshot into the ith components of each of the top k non-trivial
eigenvectors of the $M$ matrix.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Here the $M$ matrix is our anisotropic kernel. So from a spectral decomposition
of our kernel (remember that it is generated by a particular &lt;em&gt;timescale&lt;/em&gt; walk), we get a
set of eigenvectors that we project our snapshot (what we have been calling a both a trajectory frame and a sample, sorry)
that exists as a particular &lt;em&gt;timestep&lt;/em&gt; in our MD trajectory. This can create some
overly similar notation, so I’m just going to avoid it and hope that it makes more
sense without notation.&lt;/p&gt;
&lt;h2 id=&quot;using-diffusion-maps-in-mdanalysis&quot;&gt;Using Diffusion Maps in MDAnalysis&lt;/h2&gt;
&lt;p&gt;Alright, this has been a lot to digest, but hopefully you are still with me.
Why are we doing this? There are plenty of reasons, and I am going to list a few:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Dr. Ferguson used diffusion maps to investigate the assembly of polymer subunits in
&lt;a href=&quot;http://ferguson.matse.illinois.edu/resources/10.pdf&quot;&gt;this paper&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Also for the order parameters in &lt;a href=&quot;http://ferguson.matse.illinois.edu/resources/2.pdf&quot;&gt;alkane chain dynamics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Also for &lt;a href=&quot;http://ferguson.matse.illinois.edu/resources/4.pdf&quot;&gt;umbrella sampling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Dr. Clementi used this for protein folding order parameters &lt;a href=&quot;https://www.cs.rice.edu/CS/Robotics/papers/das2006low-dim-land-folding-dim-reduction.pdf&quot;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Also, Dr. Clementi used this for polymerization reactions &lt;a href=&quot;http://scitation.aip.org/content/aip/journal/jcp/134/14/10.1063/1.3575245&quot;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Dr. Clementi also created a variant that treats epsilon determination very
carefully with &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pubmed/21456654&quot;&gt;LSD&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;There are more listed in my works cited&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The first item in that list is especially cool; instead of using a standard
RMSD metric, they abstracted a cluster-matching problem into a graph matching problem,
using an algorithm called Isorank to find an approximate ‘greedy’ solution.&lt;/p&gt;
&lt;p&gt;There are some solid ‘greedy’ vs. ‘dynamic’ explanations &lt;a href=&quot;https://www.quora.com/Greedy-algorithm-vs-dynamic-programming-Whats-the-difference&quot;&gt;here&lt;/a&gt;.
The example I remember getting is to imagine you are a programmer for a GPS
direction provider. We can consider two ways of deciding an optimal route, one
with a greedy algorithm and the other with a dynamic algorithm.
At each gridpoint on a map, a greedy algorithm will take the fastest route at
that point. A dynamic algorithm will branch ahead, look into the future, and possibly
avoid short-term gain for long term drive-time savings. The greedy algorithm
might have a better best-case performance, but a much poorer worst-case performance.&lt;/p&gt;
&lt;p&gt;In any case, we want to allow for the execution of a diffusion map algorithm where
a user can provide their own metric, tune the choice of epsilon, the choice of timescale,
and project the original trajectory &lt;em&gt;timesteps&lt;/em&gt; onto the new dominant eigenvector, eigenvalue pairs.&lt;/p&gt;
&lt;h2 id=&quot;let-s-talk-api-actual-coding-hooray-&quot;&gt;Let’s talk API/ Actual Coding (HOORAY!)&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;DistMatrix&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Does frame by frame analysis on the trajectory, implements the &lt;code&gt;_prepare&lt;/code&gt;
and &lt;code&gt;_single_frame&lt;/code&gt; methods of the &lt;code&gt;BaseAnalysis&lt;/code&gt; class&lt;/li&gt;
&lt;li&gt;User selects a subset of a atoms in the trajectory here&lt;/li&gt;
&lt;li&gt;This is where user provides their own metric, cutoff for when metric is equal,
weights for weighted metric calculation, and a start, stop, step for frame
analysis&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;Epsilon&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We will have some premade classes inheriting from epsilon, but all the API
will require is to return the manipulated &lt;code&gt;DistMatrix&lt;/code&gt;, where each term has
now been divided by some scale parameter epsilon&lt;/li&gt;
&lt;li&gt;These operations should be done in place on the original &lt;code&gt;DistMatrix&lt;/code&gt;,
under no circumstances should we have two possibly large matrices sitting in
memory&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;DiffusionMap&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Accepts &lt;code&gt;DistMatrix&lt;/code&gt; (initialized), &lt;code&gt;Epsilon&lt;/code&gt; (uninitialized) with default a premade &lt;code&gt;EpsilonConstant&lt;/code&gt;
class, timescale t with default = 1, weights of anisotropic kernel as parameters&lt;/li&gt;
&lt;li&gt;Performs &lt;code&gt;BaseAnalysis&lt;/code&gt; conclude method, wherein it exponentiates to the negative of
each term given by &lt;code&gt;Epsilon.scaledMatrix&lt;/code&gt;, performs the procedure for the creation
of the anisotropic kernel above, and matrix multiplies anisotropic kernel by the timescale t.&lt;/li&gt;
&lt;li&gt;Finally, eigenvalue decomposes the anisotropic kernel and holds onto the
eigenvectors and eigenvalues as attributes.&lt;/li&gt;
&lt;li&gt;Should contain a method &lt;code&gt;DiffusionMap.embedding(timestep)&lt;/code&gt;, that projects
a timestep to its diffusion embedding at the given timescale t.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;jargon-box&quot;&gt;Jargon Box&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Crystallography&quot;&gt;Crystallography&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Metric_(mathematics&quot;&gt;Metric&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Configuration_space&quot;&gt;Configuration Space&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Random_walk&quot;&gt;Random Walk&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Markov_process&quot;&gt;Markov Process&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Diffusion_process&quot;&gt;Diffusion Process&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Fokker%E2%80%93Planck_equation&quot;&gt;Fokker-Plank Equation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Isotropy&quot;&gt;Isotropic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Anisotropy&quot;&gt;Anisotropic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Root-mean-square_deviation_of_atomic_positions&quot;&gt;RMSD&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://groups.csail.mit.edu/cb/mna/&quot;&gt;Isorank&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Works Cited:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://dip.sun.ac.za/~herbst/research/publications/diff_maps_prasa2008.pdf&quot;&gt;An Introduction to Diffusion Maps&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://cis-linux1.temple.edu/~latecki/Courses/RobotFall08/Papers/DiffusionMaps06.pdf&quot;&gt;Diffusion Maps&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pubmed/20570730&quot;&gt;Everything you wanted to know about Markov State Models but were afraid to ask&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://papers.nips.cc/paper/2942-diffusion-maps-spectral-clustering-and-eigenfunctions-of-fokker-planck-operators.pdf&quot;&gt;Diffusion Maps, Spectral Clustering and Eigenfunctions of Fokker-Planck Operators&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://ferguson.matse.illinois.edu/resources/5.pdf&quot;&gt;Nonlinear dimensionality reduction in molecular simulation: The diffusion map approach&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://ferguson.matse.illinois.edu/resources/4.pdf&quot;&gt;Integrating diffusion maps with umbrella sampling: Application to alanine&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://ferguson.matse.illinois.edu/resources/2.pdf&quot;&gt;Systematic determination of order parameters for chain dynamics using diffusion maps&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://ferguson.matse.illinois.edu/resources/10.pdf&quot;&gt;Nonlinear Machine Learning of Patchy Colloid Self-Assembly Pathways and Mechanisms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.cs.rice.edu/CS/Robotics/papers/das2006low-dim-land-folding-dim-reduction.pdf&quot;&gt;Low-Dimensional Free Energy Landscapes of Protein Folding Reactions by Nonlinear Dimensionality Reduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pubmed/21456654&quot;&gt;Determination of reaction coordinates via locally scaled diffusion map&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://scitation.aip.org/content/aip/journal/jcp/134/14/10.1063/1.3575245&quot;&gt;Polymer reversal rate calculated via locally scaled diffusion map&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://pubs.acs.org/doi/abs/10.1021/acs.jctc.6b00212&quot;&gt;Machine Learning Based Dimensionality Reduction Facilitates Ligand Diffusion Paths Assessment: A Case of Cytochrome P450cam&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://scitation.aip.org/content/aip/journal/jcp/141/11/10.1063/1.4893963&quot;&gt;Diffusion maps, clustering and fuzzy Markov modeling in peptide folding transitions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    <item>
      <title>Dimension Reduction, a review of a review</title>
      <link>/articles/dim_reduction/</link>
      <pubDate>Sun, 12 Jun 2016 17:00:00 -0700</pubDate>
      <guid isPermaLink="true">/articles/dim_reduction/</guid>
      <author></author>
      <description>&lt;script type=&quot;text/x-mathjax-config&quot;&gt;
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot; async
  src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;
&lt;/script&gt;

&lt;p&gt;Hello! This is my first post moving over to a new site built by
&lt;a href=&quot;https://github.com/jnordberg/wintersmith&quot;&gt;wintersmith&lt;/a&gt;. Originally I was going
to use jekyll pages, but there was an issue with the latest Ruby version not being
available for Linux, (maybe macs are better…). I spent &lt;em&gt;way too much&lt;/em&gt; time
figuring out how to install a markdown plugin that allowed for the inclusion of
LaTex. I did this all without realizing I could simply include:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;script type=&amp;quot;text/javascript&amp;quot; async
  src=&amp;quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&amp;quot;&amp;gt;
&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;below my article title and LaTex would easily render. Now that this roadblock is
cleared, I have no excuses preventing me from writing a post about my work.  &lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;more&quot;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This post is meant to discuss various dimension reduction methods
as a preface to a more in-depth post about diffusion maps
performed on molecular dynamics simulation trajectories. It assumes college-level math skills, but
will try to briefly explain high-level concepts from Math and Stats.
Towards the end I will provide a segue into the next post.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Dimensionality_reduction&quot;&gt;Dimension reduction&lt;/a&gt;
is performed on a data matrix $ X $ consisting of $n$ ‘samples’ wherein
each sample has a set of $m$ features associated with it. The data in the matrix
is considered to have dimension $m$, but oftentimes the actual ‘intrinsic dimensionality’
is much lower. As Laurens van der Maaten &lt;a href=&quot;https://www.tilburguniversity.edu/upload/59afb3b8-21a5-4c78-8eb3-6510597382db_TR2009005.pdf&quot;&gt;defines it&lt;/a&gt;, ‘intrinsic dimensionality’
is ‘the the minimum number of parameters needed to account for the observed properties of the data’.&lt;/p&gt;
&lt;p&gt;(So far, the most helpful explanation of this fact was presented
in a paper on diffusion maps by &lt;a href=&quot;http://dip.sun.ac.za/~herbst/research/publications/diff_maps_prasa2008.pdf&quot;&gt;Porte et al&lt;/a&gt;
In the paper, a dataset of m-by-n pixel pictures of a simple image randomly rotated
originally has dimension $mn$ but after dimension reduction, the dataset can be
organized two dimensionally based on angle of rotation.)&lt;/p&gt;
&lt;p&gt;At the most abstract level, dimension reduction methods usually are posed as an
optimization problem that often requires the solution to an eigenvalue problem.
What is an &lt;a href=&quot;https://en.wikipedia.org/wiki/Optimization_problem&quot;&gt;optimization problem&lt;/a&gt; you ask?
That wikipedia article should help some, the optimization being done in dimension
reduction is finding some linear or non-linear relation $ M $ that minimizes (or maximizes)
a cost function $ \phi (x) $ on some manipulation of the data matrix, call it $ X_{manipulated} $.
Examples of various functions will be given in detail later.&lt;/p&gt;
&lt;p&gt;In most cases this can be turned into an eigenproblem posed as:
$$ X_{manipulated} M = \lambda M $$&lt;/p&gt;
&lt;p&gt;Solving this equation using some algorithm like Singular Value Decomposition
or Symmetric Eigenvalue Decomposition will provide a set of m linearly-independent
eigenvectors that act as a basis for a lower dimensional space.
(Linear independence means no vector in the set can be expressed as
some sum of the others, a basis set has the property that any vector in a space
can be written as the sum of vectors in the set.) The set of eigenvectors is of
given by an eigenvalue decomposition will be the ‘spectrum’ of the matrix $M$.
This spectrum will have what’s referred to as a ‘spectral gap’ after a certain number
of eigenvalues, where the number of eigenvalues falls dramatically compared to the
previous. The number of significant eigenvalues before this gap reflects the
intrinsic dimension of a space.&lt;/p&gt;
&lt;p&gt;In some cases, the manipulation is somewhat more complicated, and creates what
is called a &lt;a href=&quot;https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix#Generalized_eigenvalue_problem&quot;&gt;&lt;em&gt;generalized eigenvalue problem&lt;/em&gt;&lt;/a&gt;.
In these situations the problem posed is $$ X_a M = \lambda X_b M $$
Where $X_a$ and $X_b$ are distinct but both still generated from some manipulation
on the original data matrix X.&lt;/p&gt;
&lt;p&gt;The methods discussed so far necessitate the use of convex cost functions for an optimization. From my professor Dr. Erin Pearse (thanks!):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The term convexity only make sense when discussing vector spaces, and in that case a subset U of a vector space is convex iff any convex combination of vectors in U is again in U. A convex combination is a linear combination where the coefficients are nonnegative and sum to 1.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Convex functions are similar but not entirely related. A convex function does not have any local optima that aren’t also global optima which means that if you’re at a maximum or minimum, you know it is global.&lt;/p&gt;
&lt;p&gt;(I think there is a reason why people in optimization refer to surfaces as landscapes.
An interesting surface may have many hills and valleys, and finding an optimal path
is like a hiker trying to cross a mountain path blind — potentially problematic.)&lt;/p&gt;
&lt;p&gt;Convex functions will always achieve the same solution given some input parameters,
but non-convex functions may get stuck on some local optima. This is why a method
like &lt;a href=&quot;https://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf&quot;&gt;t-SNE&lt;/a&gt; will converge to different results on different iterations.&lt;/p&gt;
&lt;p&gt;Methods for dimension reduction will be either linear or non-linear mappings.
In both cases, the original data matrix $X$ is embeddable in some manifold. A manifold
is any surface that is &lt;a href=&quot;http://planetmath.org/locallyhomeomorphic&quot;&gt;locally homeomorphic&lt;/a&gt; to $R^{2}$.
We want these mappings to preserve the local structure of the manifold, while also possibly
preserving the global structure. This depends on the task meant to be done with the reduced data. I think the notion of structure is left specifically
vague in literature because it is just so damn weird (it is really hard to think about
things in greater than 3 dimensions…)&lt;/p&gt;
&lt;p&gt;A great example of data embeddable in a weird, albeit three dimensional manifold is the Swiss roll:
&lt;img src=&quot;/articles/dim_reduction/swissroll.gif&quot; alt=&quot;swiss roll&quot;&gt; borrowed from &lt;a href=&quot;http://people.cs.uchicago.edu/~dinoj/manifold/swissroll.html&quot;&gt;dinoj&lt;/a&gt;.
The many different dimension reduction methods available will have disparate
results when performed on this data. When restricted to paths along the manifold,
red data will be far apart from black, but if a simple euclidean distance is measured,
the points might be considered close. A dimension map that uses simple euclidean
distance between points to resolve structure will fail miserably to eke out the
Swiss roll embedding.&lt;/p&gt;
&lt;p&gt;When looking to investigate the lower dimensional space created by a dimension
reduction, linear mappings have an explicit projection provided by the matrix formed
by the eigenvectors. Non-linear methods do not have such an explicit relationship.
Finding physical meaning from the order parameters given by a non-linear technique
is an active area of research.&lt;/p&gt;
&lt;p&gt;It might be too small of detail for some, but the rest of this post will be
focused on providing a quick explanation of various dimension reduction techniques.
The general format will be:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;optimization problem posed&lt;/li&gt;
&lt;li&gt;formal eigenvalue problem given&lt;/li&gt;
&lt;li&gt;interesting insights and relations&lt;/li&gt;
&lt;li&gt;pictures that I like from other work  &lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;multidimensional-scaling-mds-classical-scaling-pca&quot;&gt;Multidimensional Scaling (MDS), Classical Scaling, PCA&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;PCA cost function: Maximizes $Trace(M^{T}cov(X)M)$&lt;/li&gt;
&lt;li&gt;PCA eigenvalue problem $ Mv = \lambda v $ where M is this linear mapping minimizing
the covariance&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Quote from a &lt;a href=&quot;https://chemistry.rice.edu/FacultyDetail.aspx?RiceID=238&quot;&gt;Cecilia Clementi&lt;/a&gt; paper on diffusion maps where she mentions PCA:
‘Essentially, PCA computes a hyperplane that passes through the data points
 as best as possible in a least-squares sense. The principal components
 are the tangent vectors that describe this hyperplane’&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Classical scaling relies on the number of datapoints not the dimensionality.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;Classical scaling cost function: Minimizes $$ \phi ( Y ) = \Sigma ij =  ( d{ij} - || y{i} - y{j} ||^{2} ) $$
this is referred to as a strain cost function. (subscripts are currently an issue…)&lt;/li&gt;
&lt;li&gt;Other MDS methods can use stress or squared stress cost functions&lt;/li&gt;
&lt;li&gt;Classical scaling gives the exact same solution as PCA&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;isomap&quot;&gt;Isomap&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Geodesic distances are computed by constructing a nearest-neighbor graph and
using &lt;a href=&quot;https://www.youtube.com/watch?v=2E7MmKv0Y24&quot;&gt;Djistrka’s algorithm&lt;/a&gt; to find short distance. Erroneous connections
can be made by improperly connecting neighbors.&lt;/li&gt;
&lt;li&gt;Can fail if manifold has holes.&lt;/li&gt;
&lt;li&gt;Demonstration of failure of PCA versus success of Isomap
&lt;img src=&quot;/articles/dim_reduction/isomapfail.png&quot; alt=&quot;isomap&quot;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;kernel-pca&quot;&gt;Kernel PCA&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Does PCA on a kernel function, retains large pairwise distances even though they are measured in the feature space&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;diffusion-maps&quot;&gt;Diffusion Maps&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The key idea behind the diffusion distance is that it is based on integrating over all paths through the graph.&lt;/li&gt;
&lt;li&gt;Isomap will possibly short circuit, but the averaging of paths in diffusion maps will prevent this from happening,
it is not one shortest distance but a collective of shortest distances.&lt;/li&gt;
&lt;li&gt;Pairs of datapoints with a high forward transition probability have a small diffusion distance&lt;/li&gt;
&lt;li&gt;Eigenvalue problem: $ P^{(t)} v = \lambda v $, where $P$ is a diffusion matrix reflecting
all possible pairwise diffusion distances between two samples&lt;/li&gt;
&lt;li&gt;Diagonalization means that we can solve the equation for t=1 and then exponentiate eigenvalues
to find time solutions for longer diffusion distances&lt;/li&gt;
&lt;li&gt;Because the graph is fully connected, the largest eigenvalue is trivial&lt;/li&gt;
&lt;li&gt;The same revelation also stems from the fact that the process is markovian, that is
the step at time t only depends on the step at time t-1, it forms a &lt;a href=&quot;https://en.wikipedia.org/wiki/Markov_chain&quot;&gt;markov chain&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Molecular dynamics processes are certainly markovian, protein folding can
be modeled as a diffusion process with &lt;a href=&quot;https://en.wikipedia.org/wiki/Root-mean-square_deviation_of_atomic_positions&quot;&gt;RMSD&lt;/a&gt; as a metric&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;locally-linear-embedding-&quot;&gt;Locally Linear Embedding:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;LLE describes the local properties of the manifold around a datapoint x i by writing the datapoint
as a linear combination $w_i$ (the so-called reconstruction weights) of its k nearest-neighbors $x i_j$.&lt;/li&gt;
&lt;li&gt;It solves a generalized eigenvalue problem, preserves local structure.&lt;/li&gt;
&lt;li&gt;Invariant to local scale, rotation, translations&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Cool picture demnostrating power of LLE:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/articles/dim_reduction/LLE.jpg&quot; alt=&quot;lle&quot;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;Fails when the manifold has holes&lt;/li&gt;
&lt;li&gt;In addition, LLE tends to collapse large portions of the data very close
together in the low-dimensional space, because the covariance constraint on the solution is too simple&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;laplacian-eigenmaps-&quot;&gt;Laplacian Eigenmaps:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Laplacian Eigenmaps compute a low-dimensional representation of
the data in which the distances between a datapoint and its k nearest neighbors are minimized.&lt;/li&gt;
&lt;li&gt;The ideas studied here are a part of spectral graph theory&lt;/li&gt;
&lt;li&gt;The computation of the degree matrix M and the &lt;a href=&quot;https://en.wikipedia.org/wiki/Laplacian_matrix&quot;&gt;graph laplacian&lt;/a&gt;
L of the graph W allows for formulating the minimization problem in defined above as an eigenproblem.&lt;/li&gt;
&lt;li&gt;Generalized Eigenproblem: $Lv = \lambda Mv$&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;hessian-lle-&quot;&gt;Hessian LLE:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Minimizes curviness of the high-dimensional manifold when embedding it into
a low dimensional data representation that is &lt;a href=&quot;https://en.wikipedia.org/wiki/Isometry_(Riemannian_geometry&quot;&gt;locally isometric&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Hessian_matrix&quot;&gt;What is a Hessian?&lt;/a&gt;.
Hessian LLE uses a local hessian at every point to describe curviness.&lt;/li&gt;
&lt;li&gt;Hessian LLE shares many characteristics with Laplacian Eigenmaps:
It replaces the manifold &lt;a href=&quot;https://en.wikipedia.org/wiki/Laplacian_matrix&quot;&gt;Laplacian&lt;/a&gt; by the manifold Hessian.
‘As a result, Hessian LLE suffers from many of the same weaknesses as Laplacian Eigenmaps
and LLE.’&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;local-tangent-space-analysis-&quot;&gt;Local Tangent Space Analysis:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;LTSA simultaneously searches for the coordinates of the low-dimensional
data representations, and for the linear mappings of the low-dimensional
datapoints to the local tangent space of the high-dimensional data.&lt;/li&gt;
&lt;li&gt;Involves applying PCA on k neighbors of x before finding local tangent space&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;non-linear-methods&quot;&gt;Non-Linear Methods&lt;/h2&gt;
&lt;hr&gt;
&lt;h2 id=&quot;sammon-mapping&quot;&gt;Sammon mapping&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Adapts classical scaling by weighting the contribution of each pair $(i, j)$
to the cost function by the inverse of their pairwise distance in the high-dimensional space d_ij&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;multilayer-autoencoder&quot;&gt;Multilayer Autoencoder&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Uses a feed forward neural network that has a hidden layer with a small
number of neurons such that the neural network is forced to learn a
lower dimensional structure&lt;/li&gt;
&lt;li&gt;This is identical to PCA if using a linear activation function! What undiscovered
algorithms will be replicated by neural nets? Will neural nets actually hurt
scientific discovery?&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;Alright, so that’s all the gas that is in my tank for this post.
Hopefully you’ve come and understood something a little bit better than before.
In my next post, I am going to focus on diffusion maps as they pertain to
molecular dynamics simulations. Diffusion maps are really cool in that they
really are an analogue the physical nature of complex molecular systems.&lt;/p&gt;
&lt;h2 id=&quot;works-cited&quot;&gt;Works Cited&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://web.mit.edu/6.454/www/www_fall_2003/ihler/slides.pdf&quot;&gt;MIT Manifold Learning Slides&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.tilburguniversity.edu/upload/59afb3b8-21a5-4c78-8eb3-6510597382db_TR2009005.pdf&quot;&gt;Dimension Reduction Review&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://dip.sun.ac.za/~herbst/research/publications/diff_maps_prasa2008.pdf&quot;&gt;Diffusion Map Brief&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://web.mit.edu/6.454/www/www_fall_2003/ihler/slides.pdf&quot;&gt;MIT Slides&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf&quot;&gt;t-SNE paper&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.cs.rice.edu/CS/Robotics/papers/das2006low-dim-land-folding-dim-reduction.pdf&quot;&gt;Free Energy Landscapes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    <item>
      <title>Things I wished I had known when I got serious about programming</title>
      <link>/articles/5_things/</link>
      <pubDate>Thu, 09 Jun 2016 12:30:00 -0700</pubDate>
      <guid isPermaLink="true">/articles/5_things/</guid>
      <author></author>
      <description>&lt;p&gt; This is my first post about actual programming! When I started doing
    computational research about three years ago, I was lazy and borderline
    incompetent. Today, the tools I have learned allow me to be equally
    lazy while being somewhat more competent. These range from simple
    lifestyle decisions to basic tech skills.
&lt;/p&gt;
&lt;h2&gt; Tip 1: Install Linux &lt;/h2&gt;
&lt;p&gt;&lt;a href='https://www.linuxmint.com/download.php'&gt;
    &lt;img src='justdoit.gif' &gt;&lt;/a&gt;
    Do yourself a favor and install
    Linux. My first install of Linux was done on my laptop, and it turns
    out that the wifi was broken until I installed the new drivers. Figuring
    out why things broke was a bit of a slog, but I eventually
    stumbled upon a great demonstration of the beauty of open source software.
    Incredibly, this &lt;a href='https://github.com/lwfinger/'&gt;
    Realtek employee&lt;/a&gt; wrote drivers for Linux on his own time! &lt;a href=
    'http://askubuntu.com/questions/318608/lenovo-yoga-13-realtek-wireless-driver/358479#358479'&gt;
    Installing&lt;/a&gt; these drivers was a bit of a rabbit’s hole, but I firmly
    believe that system administration builds character. Also, for what
    its worth the install on my desktop was painless. &lt;br&gt;
    &lt;/p&gt;
&lt;h2&gt;&lt;br&gt; Tip 2: After installing linux, learn your shell commands &lt;/h2&gt;
    &lt;h3&gt; Navigating the command line&lt;/h3&gt;
    &lt;pre&gt;&lt;code&gt;
pwd # shows where you are in the filesystem
cd # changes directory to specfied path
ls # shows existing files in path
cp # copy specified file to new filename
grep # a tool to search for regular expressions or patterns inf iles/directories
mkdir # creates a directory with a specified name in current path
chmod # changes permissions for a file
sudo # if given administrator priveleges, this allows installation
# directories if given as a prefix to a shell command
rm # deletes stuff
which (command here) # shows the path taken to binary executable of a command
    &lt;/code&gt;&lt;/pre&gt;
    &lt;p&gt; Internalizing the numerous tools at ones disposal takes some time.
        Part of me thinks that I have learned command line tools simply
        because it makes me feel like Deckard. &lt;br&gt;
        &lt;div id='images'&gt;
        &lt;img src=enhance.gif&gt;&lt;/p&gt;
        &lt;/div&gt;
    &lt;p&gt; The workflow speed up can be tremendously useful. When it
        comes to tools like git, the Github Graphic User Interfaces (GUIs)
        available to Windows are &lt;em&gt;awful&lt;/em&gt;
        in comparison and it becomes a necessity. &lt;br&gt;&lt;br&gt;

    &lt;/p&gt;
    &lt;h3&gt;How I copied these downloaded gifs to my website directory&lt;/h3&gt;
    &lt;pre&gt;&lt;code&gt;
# after downloading some files, open terminal.
# change to website directory
# for me this is:
cd github/jdetle.github.io
pwd # press enter
# i am in /home/jdetlefs/github/jdetle.github.io
# trying to copy to /home/jdetlefs/github/jdetle.github.io/images
# the filename was some arbitrary string ‘YUjaCfF’ in Downloads
# when typing a path ‘~’ is a placeholder for ‘/home/username’
cp ~/Downloads/YU # (press tab in terminal to autocomplete to filename
# YU is a unique identifier for this file, pressing tab twice will list
# all files with these characters as an identifier (DONT PRESS
# ENTER YET)
(continued) images/‘filename’.gif # press enter
# check that it exists in the appropriate path
ls images
# ‘filename.gif’ should exist in this path!
    &lt;/code&gt;&lt;/pre&gt;
    &lt;p&gt; Initially this process may take longer than dragging and dropping
        files, but it quickly becomes far faster than using a GUI. &lt;/p&gt;
    &lt;h3&gt; How I add a newly installed program to my $PATH&lt;/h3&gt;
    &lt;pre&gt;&lt;code&gt;
echo $path
export PATH=$PATH:/path/to/my/program
echo $PATH
    &lt;/code&gt;&lt;/pre&gt;
    &lt;h3&gt; How I work in virtualenvs (kind of complicated) &lt;/h3&gt;
    &lt;p&gt; First, upgrade the things! &lt;/p&gt;
    &lt;pre&gt;&lt;code&gt;
sudo apt-get upgrade
    &lt;/pre&gt;&lt;/code&gt;
        From here, things can get complicated if you have an installation
        of &lt;a href='https://www.continuum.io/downloads'&gt;Anaconda.&lt;/a&gt;
        Both &lt;a href='https://pypi.python.org/pypi/pip'&gt;pip&lt;/a&gt;
        and anaconda are package managers, and but when installing itself,
        anaconda installs pip in its own directory, and handles virtual
        environments on its own. Things can break when using both conda and
        pip because managing dependencies is ugly and awful and left for
        people with higher pain tolerances than me. Of course, one can
        use conda virtualenvs, the differences probably aren’t too significant.
        This is actually still a problem on my laptop, so I am going to
        spend this time installing pip without conda and getting virtualenvs
        to work on my laptop! (&lt;a href='https://www.youtube.com/watch?v=xdZQL04sDBU'&gt;6 and a half hours later.&lt;/a&gt;)

        … Okay so this isn’t pretty. If anaconda3 is installed, it looks like
        virtualenvwrappers won’t work because virtualenvwrapper only
        works using python2.7? (Don’t hold me to this). My solution was
        to delete anaconda3 altogether. Often times I’ve learned that
        the brute force solution works pretty well. (Somewhere in the
        distance &lt;a href=https://twitter.com/pwang&gt; Peter Wang &lt;/a&gt;
        feels a disturbance in the force.)
    &lt;pre&gt;&lt;code&gt;
rm -rf anaconda3 # CAREFUL
    &lt;/pre&gt;&lt;/code&gt;
        &lt;em&gt; Be careful with this!!!&lt;/em&gt; It recursively deletes this
        directory and all files in it and can ruin your OS install.
    &lt;pre&gt;&lt;code&gt;
sudo apt-get install python-pip python-dev python-cython python-numpy g++
sudo pip install –upgrade pip
sudo pip install virtualenvwrapper
vi ~/.bashrc
    &lt;/pre&gt;&lt;/code&gt;
        &lt;p&gt; Crap! Another thing I have to explain! vi opens Vim, a text editor
            that keeps things fast and simple. A fresh installation of
            Ubuntu 14.04 will come with Vi Improved or vim, that is a
            superset of a vi (a relic of days of yore) but has no preinstalled functionality. To
            install a working version of Vim that allows for syntax highlighting
            and easy workflow tools, do the following.
    &lt;pre&gt;&lt;code&gt;
sudo apt-get update # cant hurt
sudo apt-get install vim
# ctrl-shift-n to open a new terminal window
# vi ~/.vimrc
# (press shift+colon) i (press enter) will allow you to start inserting
# copy and paste this  with a mouse
set expandtab
set tabstop=4
set softtabstop=4
set shiftwidth=3
set autoindent
set textwidth=80
set nocompatible
set backspace=2
set smartindent
set number
set cindent
colo torte # preinstalled color schemes in /usr/share/vim/vim74/colors
syntax on
# (press shift+colon) wq (press enter) to write and quit
# x does wq simultaneously
    &lt;/pre&gt;&lt;/code&gt;
    &lt;p&gt; Here is some more info on &lt;a href='http://bullium.com/support/vim.html'&gt;Vim&lt;/a&gt;.
        Let’s get back to editing our bash shell configuration file (~/.bashrc)
    &lt;/p&gt;
    &lt;pre&gt;&lt;code&gt;
vi ~/.bashrc
# before doing anything add the path
# (shift+colon) i (enter) and line 1 should be
PATH=~/bin:$PATH
# line 2
export PATH
# line 3
source /usr/local/bin/virtualenvwrapper.sh
# (shift+colon) x (enter) and (ctrl+shift+n) to open a new terminal
    &lt;/pre&gt;&lt;/code&gt;
    &lt;p&gt; And there you go! You should have a working installation of
        the &lt;a href='http://virtualenvwrapper.readthedocs.io/en/latest/'&gt;
            virtualenvwrapper&lt;/a&gt;
        such that you are ready to use virtual
        environments when making your first pull request on your new
        Linux system!
    &lt;/p&gt;
    &lt;h3&gt;Using pip virtualenvs to work on github projects&lt;/h3&gt;
    &lt;p&gt; Let’s make a pull request for MDAnalysis using the tools we’ve
        learned! &lt;/p&gt;
    &lt;pre&gt;&lt;code&gt;
# I like having a github/ folder for my various repositories
# First, let’s clone into the repo
cd (press enter) # takes us to home user directory
mkdir github
cd github
    &lt;/pre&gt;&lt;/code&gt;
    &lt;p&gt; Before moving further, you should create a github account if you
        haven’t already and fork &lt;a href='https://github.com/MDAnalysis/mdanalysis/'&gt;
        MDAnalysis&lt;/a&gt;. This will create a clone of the repo that will
        function as your ‘origin’ repository. &lt;a href='https://github.com/MDAnalysis/mdanalysis/'&gt;
        MDAnalysis&lt;/a&gt; will be the ‘upstream’ repository that we set up
        later.
    &lt;/p&gt;
    &lt;pre&gt;&lt;code&gt;
git clone &lt;a href=&quot;https://github.com/YOURUSERNAME/mdanalysis&quot;&gt;https://github.com/YOURUSERNAME/mdanalysis&lt;/a&gt;
# this takes a little bit (289 megabytes)
mkvirtualenv MDA
workon MDA
pip install numpy cython seaborn # installs dependencies
pip install -e . # installs MDAnalysis such that changing files
# changes how packages behave when loaded for a script
    &lt;/pre&gt;&lt;/code&gt;
&lt;p&gt; From here we can start working on establishing a git workflow using branches. &lt;/p&gt;
    &lt;pre&gt;&lt;code&gt;
git remote add upstream &lt;a href=&quot;http://github.com/MDAnalysis/mdanalysis&quot;&gt;http://github.com/MDAnalysis/mdanalysis&lt;/a&gt;
git branch NEW_PULL_REQUEST
git fetch upstream #checks for updates
git checkout upstream/develop -B develop # creates develop
# branch to rebase against later and switches to it
# there might be a way to do this without checking the branch out
# but I dont know how
git checkout NEW_PULL_REQUEST # do work on this branch
    &lt;/pre&gt;&lt;/code&gt;
    &lt;p&gt; Any time you want to save the work you’ve done, you can see the
        files you’ve changed with
    &lt;/p&gt;
    &lt;pre&gt;&lt;code&gt;
git status
    &lt;/code&gt;&lt;/pre&gt;
    &lt;p&gt; Then add them to be staged for a commit that will be merged into
        the upstream develop branch if the pull request is accepted.
    &lt;pre&gt;&lt;code&gt;
git add file_name_here
# once you’ve added everything you want to include in a PR
git commit -m ‘Insert a descriptive commit message here’
    &lt;/code&gt;&lt;/pre&gt;
    &lt;p&gt; If you want to make a tiny commit, and blend it into a previous commit.
    &lt;pre&gt;&lt;code&gt;
git rebase -i HEAD~(# of commits back you want to go)
    &lt;/code&gt;&lt;/pre&gt;
    &lt;p&gt; Use vim style interactiveness to rebase commits. Changing ‘pick’
        to ‘fixup’ ‘squashes’ a commit into the previous the first pick
        commit above it without using the commit statement.
        Using squash will combine commit statments. When happy, (shift+colon)
        (ctrl+x) and pressing Y and enter will combine commits.
        If still unsatisfied you can amend the commit manually.
    &lt;/p&gt;
    &lt;pre&gt;&lt;code&gt;
git commit –amend #edit the commit
    &lt;/code&gt;&lt;/pre&gt;
    &lt;p&gt; When you’re ready to save your work to the origin directory.
    &lt;pre&gt;&lt;code&gt;
git fetch upstream
git checkout develop
# if prompted:
git pull # updates changes made
# if your command prompt makes a recursive merge, you’ve done something wrong
git checkout NEW_PULL_REQUEST
git rebase develop # rebase against develop to avoid merge conflicts
git push origin NEW_PULL_REQUEST
    &lt;/code&gt;&lt;/pre&gt;
    &lt;p&gt; Before actually making a pull requestion on github, make sure
        you didn’t break any tests, and you’ve written new tests for
        the new code you’ve written.
    &lt;pre&gt;&lt;code&gt;
cd ~/github/mdanalysis/testsuite/
pip install -e .
cd MDAnalysisTests/
./mda_nosetests (press enter)
    &lt;/pre&gt;&lt;/code&gt;
        &lt;p&gt; Hopefully that helps! There is a bevy of more rigorous work
        that’s been done on understand git branching.
        &lt;a href='http://nvie.com/posts/a-successful-git-branching-model/'&gt;
            A succesful Git branching model&lt;/a&gt; is very helpful,
        &lt;a href='https://help.github.com/categories/bootcamp/'&gt;
            reading the github helps too.&lt;/a&gt;
        &lt;a href='https://atom.io/'&gt;Atom&lt;/a&gt; is a very nice editor with
        its github integration and hackability.
        I like to use &lt;a href='jupyter.org'&gt;Jupyter&lt;/a&gt;
        as a script playground for MDAnalysis.
    &lt;h2&gt; Tip 3: Get good at googling &lt;/h2&gt;
    &lt;p&gt; &lt;img src='noidea.jpg'&gt;This tip from &lt;a href='www.freecodecamp.com'&gt;freeCodeCamp&lt;/a&gt;
        is applicable to any problem. &lt;a href='https://github.com/FreeCodeCamp/wiki/blob/master/FreeCodeCamp-Get-Help.md'&gt;
        Read-search-ask&lt;/a&gt; is a strategy that will help you learn
        indepently and boost confidence. Adding on to this advice, I have
        found that if you find the email of someone knowledgable in the area
        you are struggling in, simply by writing an email explaining your problem
        you can often find the solution on your own. If you don’t
        figure it out, then you might just impress that person with
        your detailed investigation. Even if they aren’t impressed, they’ll likely help
        you out. People in open source are generally receptive
        to people who demonstrate that they are working hard at becoming self-reliant.
        Always err on the side of not sending that email though; nobody
        likes being harassed with trivial questions.
    &lt;/p&gt;
    &lt;h2&gt; Tip 4: When working, avoid distractions, double check,
        triple check, quadruple check… &lt;/h2&gt;
        &lt;p&gt; When working on projects involving non-commercial software
            it is especially important to think of all the possible ways you could
            have screwed something up. Check your code for glaring logic
            errors and before running an intensive calculation, run a baseline
            to ensure that things work. In quantum chemistry, an example
            for this would be running a Hartree-Fock calculation with the STO-3G basis set
            before doing something that scales much slower.
            Develop scripts to ensure you are getting expected results,
            become skilled at using grep and simple regex. (&lt;a href='http://regexr.com/'&gt;Regexr&lt;/a&gt;
            is a great playground to learn regex)
            Assume that you’ve written bad code and that bugs will
            be caused by small changes to input parameters. Expect
            things to break easily. Inspect all work exhaustively.
        &lt;/p&gt;
        &lt;p&gt; When reading academic papers, print them out and read them away from a PC.
            Usually academic papers use wildly esoteric jargon. &lt;a href='http://ferguson.matse.illinois.edu/resources/5.pdf'&gt;
            This paper&lt;/a&gt; on diffusion maps (the subject of my next blog post)
            actually features a ‘jargon box’ which is just &lt;em&gt;great&lt;/em&gt;. Academic
            papers usually also assume a high level of familiarity in the
            subject material and are written for those who are skilled at
            reading papers. It is easier to dedicate the intense
            concentration required for most papers when unplugging from tech
            and using some ear plugs.
        &lt;/p&gt;
        &lt;p&gt; Finally, when communicating over email you can embrace
            one of two strategies. Either you can add a ‘sent from my iPhone’
            tag to everything, or before adding recipients,
            take a second to go get a drink of water come back and
            reread the message for errors. Unfortunately, people will
            judge you for poor grammar even if they don’t mean to. (Shoot,
            I just ended a sentence in a preposition…)
        &lt;/p&gt;
    &lt;h2&gt; Tip 5: Tackle what intimidates you &lt;/h2&gt;
        &lt;p&gt; I seriously believe that this is the number one part of
            becoming an adult and it is something I have only really
            internalized in the last year. Problems will not go away
            by avoiding them. Oftentimes I find myself building up things
            in my head as if they will be a bigger deal than they actually
            are. Figuring out how to us virtualenvs was one example of
            such a barrier that occurred recently. This occurs in my personal
            life as well and invariably the outcome is always better than
            how I imagined it would be.
        &lt;/p&gt;&lt;img src='shiacrying.gif'&gt;
        &lt;p&gt; Having trouble getting started on a project? Unfortunately Shia isn’t
            much help here.
            Segment your work into discrete chunks. If you
            have a pull request you want to make, think of all the
            possible minutia you have to work through in order to get
            things done. I like to use Google Inbox’s reminder feature
            to constantly remind myself of these things I need to get done.
            When I finish a task, I can swipe it off my todo list and
            enjoy that feeling of catharsis.
        &lt;/p&gt;
        &lt;p&gt; If you are a budding programmer, take an algorithms class
            for free &lt;a href='http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-006-introduction-to-algorithms-fall-2011/'&gt;here&lt;/a&gt;.
            If you still aren’t busy enough, read the MDAnalysis &lt;a href='https://github.com/MDAnalysis/mdanalysis/wiki/Guide-for-Developers'&gt;Guide
            for Developers&lt;/a&gt; and start learning with help from a tight-knit
            community of open source contributors.
        &lt;/p&gt;
</description>
    </item>
    <item>
      <title>Hello World</title>
      <link>/articles/hello_world/</link>
      <pubDate>Wed, 11 May 2016 12:30:00 -0700</pubDate>
      <guid isPermaLink="true">/articles/hello_world/</guid>
      <author></author>
      <description>&lt;p&gt;Hello world! I recently was given the amazing opportunity
    to contribute to MDAnalysis, an open source Molecular Dynamics simulation
    Analysis project through the Google Summer of Code initiative. I’ve
    been encouraged to maintain a blog by those giving me this opportunity
    so I’ll start things off by explaining how I got this great summer job.
&lt;/p&gt;
&lt;span class=&quot;more&quot;&gt;&lt;/span&gt;
&lt;p&gt;To summarize it quickly, Google sponsors a program in which college
    students apply to work on projects for open source software organizations.
    I was very lucky to have my research advisor, Dr. Ashley Ringer
    McDonald, encourage me to apply. I satisfied the first application requirement
    by learning how to use git and closing an issue on the &lt;a href=&quot;https://github.com/MDAnalysis/mdanalysis/issues&quot;&gt;MDAnalysis Github page&lt;/a&gt;.
    After that, I spent about 40 hours of concentrated effort over spring break
    studying dimensionality reduction and molecular dynamics in order to write a coherent
    application. By turning in a rough draft early I ensured the process was iterative; the contributors to
    MDAnalysis were very helpful with their critiques of my application.
&lt;/p&gt;
&lt;p&gt; After turning in my final application, the process didn’t really stop. I made
    sure to keep making pull requests and to learn more about development workflow. I have
    learned &lt;strong&gt;so&lt;/strong&gt; much about workflow and how to get over
    the dread of starting a pull request in the past few months.
&lt;/p&gt;
&lt;p&gt; And then I got the news! I had been accepted to the Google Summer of Code!
    I was and still am extremely excited. With that being said, success
    made me lazy and somewhat complacent. Recently, I have been doing the
    bare minimum in terms of work and that is about to change. Even if
    no one is reading this, consider this blog as the first step in accountability
    for the rest of the summer. I will be using this to keep a record of everything I
    am working on day to day.
&lt;/p&gt;
&lt;p&gt; With the exception of this introduction post, every post will attempt to
    keep a focus on a particular issue. I might write a post about a
    topic related to my Summer of Code work, or something related to
    my many other interests. I endeavor to remain positive and
    thoughtful, I will work on my clear overuse of commas, and I will
    try to make my readers laugh.
&lt;/p&gt;
&lt;p&gt; I look forward to keeping this up! JD out. &lt;/p&gt;
</description>
    </item>
  </channel>
</rss>